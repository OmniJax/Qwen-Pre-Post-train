{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRPO 教学（手写公式 + 最小实现，无 LoRA）\n",
        "\n",
        "本 Notebook 目标：像之前的 DPO/PPO 教学一样，用 PyTorch + Transformers 手写 GRPO（Group Relative Policy Optimization）的关键计算：\n",
        "\n",
        "- 写清楚公式\n",
        "- 代码变量名/计算过程能对应公式\n",
        "- 不使用 TRL 这类现成 RLHF Trainer\n",
        "\n",
        "注意：无 LoRA 表示全参更新，对显存要求更高；如果你只有 8GB，务必把 `group_size/max_new_tokens/grpo_epochs` 调小。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. GRPO 的核心思想与公式\n",
        "\n",
        "GRPO 可以理解为：**PPO 的一个“去 critic”变体**。\n",
        "\n",
        "- PPO 通常需要 critic（$V(s)$）来估计优势函数 $A_t$（如 GAE）。\n",
        "- GRPO 用“同一 prompt 下的一组采样结果”做相对基线：在一个 group 内把奖励做中心化/标准化，得到序列级 advantage，从而不需要训练价值网络。\n",
        "\n",
        "### 1.1 采样与奖励\n",
        "\n",
        "对一个 prompt $x$，用旧策略（rollout 时的策略）$\\pi_{\\theta_{old}}$ 采样 $G$ 个回答：\n",
        "\n",
        "$$\n",
        "y_i \\sim \\pi_{\\theta_{old}}(\\cdot\\mid x),\\quad i=1,\\dots,G\n",
        "$$\n",
        "\n",
        "每个回答 $y_i$ 有 token 序列 $a_{i,1:T_i}$，并得到一个**序列级**奖励（来自 RM 或规则）：\n",
        "\n",
        "$$\n",
        "r_i = r(x, y_i)\n",
        "$$\n",
        "\n",
        "### 1.2 Group-relative Advantage（去掉 critic 的关键）\n",
        "\n",
        "在同一个 group 内做标准化（也可以只做中心化）：\n",
        "\n",
        "$$\n",
        "\\mu = \\frac{1}{G}\\sum_{i=1}^{G} r_i,\\quad \\sigma = \\sqrt{\\frac{1}{G}\\sum_{i=1}^{G}(r_i-\\mu)^2}+\\varepsilon\n",
        "$$\n",
        "\n",
        "$$\n",
        "A_i = \\frac{r_i-\\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "这里 $A_i$ 是**序列级** advantage（对同一个回答的所有 token 都一样）。\n",
        "\n",
        "\n",
        "在纯粹的 GRPO 算法设计中，没有任何针对“中间推导过程是否正确”的奖励机制。\n",
        "\n",
        "GRPO 是一种极度纯粹的 **“结果导向（Outcome-supervised）”** 算法。\n",
        "\n",
        "### 1.3 PPO 的 ratio 与 clipped surrogate（token 级）\n",
        "\n",
        "每个 token 的对数概率：\n",
        "\n",
        "$$\n",
        "\\log\\pi_{\\theta}(a_{i,t}\\mid s_{i,t}),\\quad s_{i,t}=(x, a_{i,<t})\n",
        "$$\n",
        "\n",
        "概率比值（PPO 核心）：\n",
        "\n",
        "$$\n",
        "\\rho_{i,t}(\\theta)=\\frac{\\pi_{\\theta}(a_{i,t}\\mid s_{i,t})}{\\pi_{\\theta_{old}}(a_{i,t}\\mid s_{i,t})}\n",
        "=\\exp\\big(\\log\\pi_{\\theta}-\\log\\pi_{\\theta_{old}}\\big)\n",
        "$$\n",
        "\n",
        "clipped surrogate（对每个 token）：\n",
        "\n",
        "$$\n",
        "L^{clip}_{i,t}(\\theta)=\\min\\Big(\\rho_{i,t}(\\theta)A_i,\\;\\mathrm{clip}(\\rho_{i,t}(\\theta),1-\\epsilon,1+\\epsilon)A_i\\Big)\n",
        "$$\n",
        "\n",
        "对 token 平均、对 group 平均：\n",
        "\n",
        "$$\n",
        "L^{clip}(\\theta)=\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{T_i}\\sum_{t=1}^{T_i} L^{clip}_{i,t}(\\theta)\n",
        "$$\n",
        "\n",
        "### 1.4 KL 约束（对参考策略 $\\pi_{ref}$）\n",
        "\n",
        "常见 RLHF 会加 KL 约束让策略不要偏离参考模型（冻结）：\n",
        "\n",
        "$$\n",
        "\\widehat{KL}_{i,t}=\\log\\pi_{\\theta}(a_{i,t}\\mid s_{i,t})-\\log\\pi_{ref}(a_{i,t}\\mid s_{i,t})\n",
        "$$\n",
        "\n",
        "把 KL 作为惩罚项（系数 $\\beta$）：\n",
        "\n",
        "$$\n",
        "J(\\theta)=L^{clip}(\\theta) - \\beta\\cdot \\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{T_i}\\sum_{t=1}^{T_i}\\widehat{KL}_{i,t}\n",
        "$$\n",
        "\n",
        "训练时最小化 loss：\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = -J(\\theta)\n",
        "$$\n",
        "\n",
        "下面代码会用同名变量：`rewards/advantages/logpi_old/logpi/logpi_ref/ratio/clip_eps/kl_coef` 对应上面公式逐项实现。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 环境与模型加载（建议离线）\n",
        "\n",
        "- 建议 `conda activate llm`\n",
        "- 默认优先用 `MODELSCOPE_CACHE` 下的本地模型目录（避免联网）\n",
        "- 参考模型默认放 CPU（更省显存，但更慢；显存够可放 GPU）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python: e:\\Softwares\\anaconda3\\envs\\llm\\python.exe\n",
            "torch: 2.10.0+cu126 cuda: True\n",
            "MODELSCOPE_CACHE: D:/myProject/modelscope_hub\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "os.environ.setdefault(\"MODELSCOPE_CACHE\", r\"D:/myProject/modelscope_hub\")\n",
        "print(\"python:\", sys.executable)\n",
        "print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "print(\"MODELSCOPE_CACHE:\", os.environ[\"MODELSCOPE_CACHE\"])\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name_or_path: D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0___5B-Instruct\n",
            "dtype: torch.bfloat16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ready\n"
          ]
        }
      ],
      "source": [
        "# 选择模型（优先本地缓存目录）\n",
        "local_dir = Path(os.environ[\"MODELSCOPE_CACHE\"]) / \"models\" / \"qwen\" / \"Qwen2-0___5B-Instruct\"\n",
        "model_name_or_path = str(local_dir) if local_dir.exists() else \"qwen/Qwen2-0.5B-Instruct\"\n",
        "print(\"model_name_or_path:\", model_name_or_path)\n",
        "\n",
        "if device == \"cuda\":\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "else:\n",
        "    dtype = torch.float32\n",
        "print(\"dtype:\", dtype)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = \"<|endoftext|>\"\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
        "\n",
        "# 可训练策略模型 π_θ\n",
        "policy = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype).to(device)\n",
        "policy.config.use_cache = False\n",
        "policy.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "if hasattr(policy, \"gradient_checkpointing_enable\") and device == \"cuda\":\n",
        "    policy.gradient_checkpointing_enable()  # 省显存\n",
        "\n",
        "# 参考策略 π_ref（冻结）\n",
        "ref_device = \"cpu\"  # 显存够可以改成 \"cuda\"\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32).to(ref_device)\n",
        "ref_model.eval()\n",
        "for p in ref_model.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "print(\"ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Toy 任务与规则奖励 $r(x,y)$\n",
        "\n",
        "真实 GRPO/RLHF 会用 reward model（RM）。教学版这里用规则奖励：检查输出是否包含期望字符串。\n",
        "\n",
        "奖励（可自行改）：\n",
        "\n",
        "- 命中期望：+1\n",
        "- 不命中：-1\n",
        "- 长度轻微惩罚（鼓励更短）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': 'czq是谁', 'expected': 'czq是神！'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_tasks = [\n",
        "    {\"prompt\": \"czq是谁\", \"expected\": \"czq是神！\"},\n",
        "    {\"prompt\": \"请只输出数字 4，不要额外文字。2+2等于几？\", \"expected\": \"4\"},\n",
        "    {\"prompt\": \"把“我喜欢机器学习”翻译成英文，只输出翻译。\", \"expected\": \"I like machine learning\"},\n",
        "    {\"prompt\": \"请只回答：通义千问\", \"expected\": \"通义千问\"},\n",
        "]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    for ch in [\" \", \"\\n\", \"\\t\", \"。\", \"，\", \",\", \".\", \"!\", \"?\", \"：\", \":\", \"\\\"\", \"'\"]:\n",
        "        s = s.replace(ch, \"\")\n",
        "    return s\n",
        "\n",
        "def rule_reward(prompt: str, response: str, expected: str) -> float:\n",
        "    resp = normalize_text(response)\n",
        "    exp = normalize_text(expected)\n",
        "    hit = exp in resp\n",
        "    base = 1.0 if hit else -1.0\n",
        "    length_penalty = 0.002 * len(resp)\n",
        "    return base - length_penalty\n",
        "\n",
        "train_tasks[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 关键实现：采样 group、计算 logprob/ratio、GRPO loss\n",
        "\n",
        "实现要点：\n",
        "\n",
        "- `logπ(a|s)` 用 `log_softmax(logits)` + `gather` 手写（贴公式）\n",
        "- `logpi_old` 在 rollout 时固定\n",
        "- `logpi_ref` 用冻结参考模型计算\n",
        "- advantage 用 group 标准化得到 `A_i`，再广播到每个 token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def build_prompt_input_ids(tok: Any, user_prompt: str) -> torch.Tensor:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "    text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    enc = tok(text, return_tensors=\"pt\")\n",
        "    return enc[\"input_ids\"][0]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def sample_group(\n",
        "    m: torch.nn.Module,\n",
        "    tok: Any,\n",
        "    prompt_ids: torch.Tensor,\n",
        "    group_size: int,\n",
        "    max_new_tokens: int,\n",
        "    temperature: float,\n",
        "    top_p: float,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    m.eval()\n",
        "    prompt_len = int(prompt_ids.numel())\n",
        "    input_ids = prompt_ids.unsqueeze(0).to(device)\n",
        "\n",
        "    old_use_cache = getattr(m.config, \"use_cache\", True)\n",
        "    m.config.use_cache = True\n",
        "    out = m.generate(\n",
        "        input_ids=input_ids,\n",
        "        num_return_sequences=group_size,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "    )\n",
        "    m.config.use_cache = old_use_cache\n",
        "\n",
        "    seqs = out.detach().cpu()\n",
        "    samples: List[Dict[str, Any]] = []\n",
        "    for seq in seqs:\n",
        "        resp = seq[prompt_len:]\n",
        "        # trim at eos/pad\n",
        "        end = int(resp.numel())\n",
        "        for j, tid in enumerate(resp.tolist()):\n",
        "            if tid == tok.eos_token_id or tid == tok.pad_token_id:\n",
        "                end = j\n",
        "                break\n",
        "        resp_ids = resp[:end]\n",
        "        if resp_ids.numel() == 0:\n",
        "            continue\n",
        "        full_ids = torch.cat([prompt_ids, resp_ids], dim=0)\n",
        "        text = tok.decode(resp_ids, skip_special_tokens=True)\n",
        "        samples.append(\n",
        "            {\n",
        "                \"full_ids\": full_ids,\n",
        "                \"response_ids\": resp_ids,\n",
        "                \"response_text\": text,\n",
        "                \"response_len\": int(resp_ids.numel()),\n",
        "            }\n",
        "        )\n",
        "    return samples\n",
        "\n",
        "def pad_group(prompt_ids: torch.Tensor, samples: List[Dict[str, Any]], pad_id: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int, int]:\n",
        "    prompt_len = int(prompt_ids.numel())\n",
        "    max_resp_len = max(s[\"response_len\"] for s in samples)\n",
        "    B = len(samples)\n",
        "\n",
        "    input_ids = torch.full((B, prompt_len + max_resp_len), pad_id, dtype=torch.long)\n",
        "    attention_mask = torch.zeros_like(input_ids, dtype=torch.long)\n",
        "    response_mask = torch.zeros((B, max_resp_len), dtype=torch.bool)\n",
        "\n",
        "    for i, s in enumerate(samples):\n",
        "        resp = s[\"response_ids\"]\n",
        "        rlen = int(resp.numel())\n",
        "        input_ids[i, :prompt_len] = prompt_ids\n",
        "        input_ids[i, prompt_len : prompt_len + rlen] = resp\n",
        "        attention_mask[i, : prompt_len + rlen] = 1\n",
        "        response_mask[i, :rlen] = True\n",
        "\n",
        "    return input_ids, attention_mask, response_mask, prompt_len, max_resp_len\n",
        "\n",
        "def action_logp_entropy_from_logits(\n",
        "    logits: torch.Tensor,\n",
        "    input_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    max_resp_len: int,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"返回 response token 的 (logπ, entropy)。\n",
        "\n",
        "    logits: (B, L, V)\n",
        "    input_ids: (B, L)\n",
        "    \"\"\"\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    # 位置 p-1 预测 input_ids[p]\n",
        "    log_probs_slice = log_probs[:, prompt_len - 1 : prompt_len - 1 + max_resp_len, :]\n",
        "    action_ids = input_ids[:, prompt_len : prompt_len + max_resp_len]\n",
        "\n",
        "    token_logp = log_probs_slice.gather(dim=-1, index=action_ids.unsqueeze(-1)).squeeze(-1)\n",
        "    entropy = -(log_probs_slice.exp() * log_probs_slice).sum(dim=-1)\n",
        "    return token_logp, entropy\n",
        "\n",
        "def grpo_loss(\n",
        "    logpi: torch.Tensor,\n",
        "    logpi_old: torch.Tensor,\n",
        "    logpi_ref: torch.Tensor,\n",
        "    advantages: torch.Tensor,\n",
        "    response_mask: torch.Tensor,\n",
        "    clip_eps: float,\n",
        "    kl_coef: float,\n",
        "    ent_coef: float,\n",
        "    entropy: torch.Tensor,\n",
        ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "    \"\"\"对应：\n",
        "    ratio = exp(logπ - logπ_old)\n",
        "    L_clip = mean(min(ratio*A, clip(ratio)*A))\n",
        "    loss = -L_clip + kl_coef*mean(logπ - logπ_ref) - ent_coef*mean(entropy)\n",
        "    \"\"\"\n",
        "    mask = response_mask.to(device=logpi.device, dtype=logpi.dtype)\n",
        "    denom = mask.sum().clamp_min(1.0)\n",
        "\n",
        "    A = advantages.to(dtype=logpi.dtype).unsqueeze(1)\n",
        "    ratio = torch.exp(logpi - logpi_old)\n",
        "    surr1 = ratio * A\n",
        "    surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * A\n",
        "    clipped_obj = torch.min(surr1, surr2)\n",
        "    policy_loss = -(clipped_obj * mask).sum() / denom\n",
        "\n",
        "    kl = logpi - logpi_ref\n",
        "    kl_mean = (kl * mask).sum() / denom\n",
        "    kl_loss = kl_coef * kl_mean\n",
        "\n",
        "    entropy_mean = (entropy * mask).sum() / denom\n",
        "    loss = policy_loss + kl_loss - ent_coef * entropy_mean\n",
        "\n",
        "    clip_frac = (((ratio - 1.0).abs() > clip_eps).to(logpi.dtype) * mask).sum() / denom\n",
        "\n",
        "    metrics = {\n",
        "        \"policy_loss\": policy_loss.detach(),\n",
        "        \"kl_mean\": kl_mean.detach(),\n",
        "        \"entropy\": entropy_mean.detach(),\n",
        "        \"clip_frac\": clip_frac.detach(),\n",
        "        \"loss\": loss.detach(),\n",
        "    }\n",
        "    return loss, metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. GRPO 训练循环（教学版）\n",
        "\n",
        "每一步训练：\n",
        "\n",
        "1) 选一个 prompt，采样 `group_size` 个回答\n",
        "2) 用规则得到每个回答的 reward：`rewards[i] = r(x,y_i)`\n",
        "3) 在 group 内标准化得到 `advantages[i]`\n",
        "4) 计算 `logpi_old`（固定）与 `logpi_ref`（固定）\n",
        "5) 做 `grpo_epochs` 次更新：用当前模型算 `logpi`，计算 `ratio`、clipped objective、KL penalty，反向传播\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jax\\AppData\\Local\\Temp\\ipykernel_83268\\2902804714.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and not use_bf16)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step=0 reward_mean=-1.104 reward_std=0.029 loss=0.2606 policy=0.2618 kl=-0.0246 clip_frac=0.033\n",
            "prompt: czq是谁\n",
            "best_reward: -1.0679999589920044 response: 对不起，我需要更多的信息才能回答这个问题。你可以告诉我你需要什么帮助吗？\n",
            "------------------------------------------------------------\n",
            "step=1 reward_mean=-1.113 reward_std=0.034 loss=0.2399 policy=0.2400 kl=-0.0014 clip_frac=0.007\n",
            "prompt: czq是谁\n",
            "best_reward: -1.0720000267028809 response: 抱歉，我不太明白你的意思。能否提供更多的背景信息或细节？我将尽我所能来帮助你。\n",
            "------------------------------------------------------------\n",
            "step=2 reward_mean=0.456 reward_std=0.872 loss=-0.0072 policy=-0.0071 kl=-0.0020 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "best_reward: 0.9599999785423279 response: \"I like machine learning\"\n",
            "------------------------------------------------------------\n",
            "step=3 reward_mean=0.992 reward_std=0.004 loss=0.4025 policy=0.4030 kl=-0.0118 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "best_reward: 0.9980000257492065 response: 4\n",
            "------------------------------------------------------------\n",
            "step=4 reward_mean=0.487 reward_std=0.863 loss=-0.2024 policy=-0.2022 kl=-0.0046 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "best_reward: 0.9900000095367432 response: 2 + 2 = 4\n",
            "------------------------------------------------------------\n",
            "step=5 reward_mean=0.493 reward_std=0.864 loss=-0.2714 policy=-0.2713 kl=-0.0007 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "best_reward: 0.9980000257492065 response: 4\n",
            "------------------------------------------------------------\n",
            "step=6 reward_mean=-1.106 reward_std=0.030 loss=0.2728 policy=0.2733 kl=-0.0109 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "best_reward: -1.0820000171661377 response: 很抱歉，我不能确定这是谁。如果你能提供更多的信息或者背景，请让我知道，我会尽力回答你的问题。\n",
            "------------------------------------------------------------\n",
            "step=7 reward_mean=-1.120 reward_std=0.041 loss=0.3752 policy=0.3753 kl=-0.0020 clip_frac=0.007\n",
            "prompt: czq是谁\n",
            "best_reward: -1.0499999523162842 response: 对不起，我没有找到这个人的相关信息。我无法回答这个问题。\n",
            "------------------------------------------------------------\n",
            "step=8 reward_mean=-0.074 reward_std=0.968 loss=-0.4493 policy=-0.4491 kl=-0.0056 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "best_reward: 0.9200000166893005 response: 我是通义千问，是一款基于阿里云自主研发的超大规模语言模型。请问有什么可以帮助你的呢？\n",
            "------------------------------------------------------------\n",
            "step=9 reward_mean=-1.082 reward_std=0.055 loss=0.6079 policy=0.6076 kl=0.0075 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "best_reward: -1.0199999809265137 response: 抱歉，我不太知道这是谁。\n",
            "------------------------------------------------------------\n",
            "step=10 reward_mean=-1.112 reward_std=0.034 loss=0.3354 policy=0.3360 kl=-0.0110 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "best_reward: -1.0579999685287476 response: 对不起，我不了解这个人。请告诉我更多的信息或者使用其他语言提问。\n",
            "------------------------------------------------------------\n",
            "step=11 reward_mean=-1.103 reward_std=0.036 loss=0.3165 policy=0.3173 kl=-0.0160 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "best_reward: -1.0540000200271606 response: 我无法确定您是指谁。请您提供更多信息，我会尽力回答您的问题。\n",
            "------------------------------------------------------------\n",
            "step=12 reward_mean=0.986 reward_std=0.003 loss=0.1590 policy=0.1589 kl=0.0034 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "best_reward: 0.9879999756813049 response: 2+2等于4\n",
            "------------------------------------------------------------\n",
            "step=13 reward_mean=-0.006 reward_std=0.999 loss=-0.1754 policy=-0.1724 kl=-0.0593 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "best_reward: 0.9980000257492065 response: 4\n",
            "------------------------------------------------------------\n",
            "step=14 reward_mean=-1.116 reward_std=0.040 loss=0.3047 policy=0.3047 kl=0.0003 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "best_reward: -1.0720000267028809 response: 对不起，我无法直接联系或获取个人身份信息。如果您有其他问题需要帮助，请告诉我！\n",
            "------------------------------------------------------------\n",
            "step=15 reward_mean=0.993 reward_std=0.006 loss=0.7658 policy=0.7663 kl=-0.0094 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "best_reward: 0.9980000257492065 response: 4\n",
            "------------------------------------------------------------\n",
            "step=16 reward_mean=-0.064 reward_std=0.992 loss=-0.1628 policy=-0.1626 kl=-0.0052 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "best_reward: 0.9319999814033508 response: “通义千问”是百度推出的AI聊天机器人，用户可以与它进行实时的问答对话。\n",
            "------------------------------------------------------------\n",
            "step=17 reward_mean=-0.006 reward_std=0.999 loss=-0.4309 policy=-0.4270 kl=-0.0778 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "best_reward: 0.9980000257492065 response: 4\n",
            "------------------------------------------------------------\n",
            "step=18 reward_mean=0.370 reward_std=0.845 loss=-0.1876 policy=-0.1873 kl=-0.0052 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "best_reward: 0.878000020980835 response: 非常抱歉，但我无法提供关于通义千问的信息。我的目标是尽可能为用户提供帮助和解答相关问题。如果你有任何其他需要了解的问题，请随时提问。\n",
            "------------------------------------------------------------\n",
            "step=19 reward_mean=-0.038 reward_std=0.996 loss=-0.0124 policy=-0.0130 kl=0.0127 clip_frac=0.050\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "best_reward: 0.9599999785423279 response: \"I like machine learning\"\n",
            "------------------------------------------------------------\n",
            "step=20 reward_mean=-1.077 reward_std=0.040 loss=0.4687 policy=0.4678 kl=0.0169 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "best_reward: -1.0199999809265137 response: 抱歉，我不太知道他是谁。\n",
            "------------------------------------------------------------\n",
            "step=21 reward_mean=0.491 reward_std=0.862 loss=-0.4966 policy=-0.4940 kl=-0.0508 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "best_reward: 0.9900000095367432 response: 2 + 2 = 4\n",
            "------------------------------------------------------------\n",
            "step=22 reward_mean=0.864 reward_std=0.034 loss=0.1997 policy=0.2001 kl=-0.0085 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "best_reward: 0.9120000004768372 response: 我不会进行任何形式的自我介绍或提供关于通义千问的信息。如果您有任何问题，我会尽我所能来帮助您。\n",
            "------------------------------------------------------------\n",
            "step=23 reward_mean=0.960 reward_std=0.000 loss=0.0043 policy=-0.0000 kl=0.0868 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "best_reward: 0.9599999785423279 response: \"I like machine learning\"\n",
            "------------------------------------------------------------\n",
            "step=24 reward_mean=0.447 reward_std=0.874 loss=0.3858 policy=0.3843 kl=0.0298 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "best_reward: 0.9599999785423279 response: \"I like Machine Learning\"\n",
            "------------------------------------------------------------\n",
            "step=25 reward_mean=0.993 reward_std=0.005 loss=0.8355 policy=0.8341 kl=0.0282 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "best_reward: 0.9980000257492065 response: 4\n",
            "------------------------------------------------------------\n",
            "step=26 reward_mean=0.497 reward_std=0.865 loss=-0.3032 policy=-0.2948 kl=-0.1685 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "best_reward: 0.9980000257492065 response: 4\n",
            "------------------------------------------------------------\n",
            "step=27 reward_mean=0.462 reward_std=0.862 loss=-0.3093 policy=-0.3114 kl=0.0410 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "best_reward: 0.9599999785423279 response: \"I like machine learning\"\n",
            "------------------------------------------------------------\n",
            "step=28 reward_mean=-1.076 reward_std=0.014 loss=0.2095 policy=0.2095 kl=-0.0016 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "best_reward: -1.059999942779541 response: 很抱歉，我不了解您的问题。如果您需要帮助，请告诉我您想问的具体问题。\n",
            "------------------------------------------------------------\n",
            "step=29 reward_mean=-1.077 reward_std=0.048 loss=0.5661 policy=0.5654 kl=0.0132 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "best_reward: -1.0240000486373901 response: 对不起，我不太清楚这是什么。\n",
            "------------------------------------------------------------\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "# 超参（教学默认）\n",
        "train_steps = 30\n",
        "group_size = 4\n",
        "max_new_tokens = 48\n",
        "temperature = 1.0\n",
        "top_p = 0.9\n",
        "\n",
        "clip_eps = 0.2\n",
        "kl_coef = 0.05\n",
        "ent_coef = 0.0\n",
        "grpo_epochs = 2\n",
        "\n",
        "lr = 2e-6  # 全参更新，建议很小\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "optimizer = torch.optim.AdamW(policy.parameters(), lr=lr)\n",
        "\n",
        "use_amp = device == \"cuda\"\n",
        "use_bf16 = use_amp and torch.cuda.is_bf16_supported()\n",
        "autocast_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "autocast_device_type = \"cuda\" if use_amp else \"cpu\"\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp and not use_bf16)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def compute_logpi_ref(\n",
        "    ref: torch.nn.Module,\n",
        "    input_ids: torch.Tensor,\n",
        "    attention_mask: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    max_resp_len: int,\n",
        ") -> torch.Tensor:\n",
        "    ids = input_ids.to(ref_device)\n",
        "    mask = attention_mask.to(ref_device)\n",
        "    logits = ref(input_ids=ids, attention_mask=mask, use_cache=False, return_dict=True).logits\n",
        "    logp, _ = action_logp_entropy_from_logits(logits, ids, prompt_len, max_resp_len)\n",
        "    return logp.to(device)\n",
        "\n",
        "print(\"start training\")\n",
        "for step in range(train_steps):\n",
        "    task = random.choice(train_tasks)\n",
        "    prompt = task[\"prompt\"]\n",
        "    expected = task[\"expected\"]\n",
        "\n",
        "    prompt_ids = build_prompt_input_ids(tokenizer, prompt)\n",
        "    samples = sample_group(policy, tokenizer, prompt_ids, group_size, max_new_tokens, temperature, top_p)\n",
        "    if len(samples) < 2:\n",
        "        print(f\"step={step} skip(group too small)\")\n",
        "        continue\n",
        "\n",
        "    rewards = [rule_reward(prompt, s[\"response_text\"], expected) for s in samples]\n",
        "    rewards_t = torch.tensor(rewards, device=device, dtype=torch.float32)\n",
        "    advantages = (rewards_t - rewards_t.mean()) / (rewards_t.std(unbiased=False).clamp_min(1e-6))\n",
        "\n",
        "    input_ids, attention_mask, response_mask, prompt_len, max_resp_len = pad_group(\n",
        "        prompt_ids, samples, pad_id=tokenizer.pad_token_id\n",
        "    )\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    # logπ_old（rollout 时固定）\n",
        "    with torch.inference_mode():\n",
        "        policy.eval()\n",
        "        logits_old = policy(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, return_dict=True).logits\n",
        "        logpi_old, _ = action_logp_entropy_from_logits(logits_old, input_ids, prompt_len, max_resp_len)\n",
        "        logpi_old = logpi_old.detach()\n",
        "\n",
        "    # logπ_ref（冻结参考模型）\n",
        "    logpi_ref = compute_logpi_ref(ref_model, input_ids, attention_mask, prompt_len, max_resp_len).detach()\n",
        "\n",
        "    # 多轮 GRPO 更新\n",
        "    policy.train()\n",
        "    for ep in range(grpo_epochs):\n",
        "        with torch.autocast(device_type=autocast_device_type, dtype=autocast_dtype, enabled=use_amp):\n",
        "            logits = policy(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, return_dict=True).logits\n",
        "            logpi, entropy = action_logp_entropy_from_logits(logits, input_ids, prompt_len, max_resp_len)\n",
        "            loss, m = grpo_loss(\n",
        "                logpi=logpi,\n",
        "                logpi_old=logpi_old,\n",
        "                logpi_ref=logpi_ref,\n",
        "                advantages=advantages,\n",
        "                response_mask=response_mask,\n",
        "                clip_eps=clip_eps,\n",
        "                kl_coef=kl_coef,\n",
        "                ent_coef=ent_coef,\n",
        "                entropy=entropy,\n",
        "            )\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if scaler.is_enabled():\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(policy.parameters(), max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(policy.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "    # 日志\n",
        "    print(\n",
        "        f\"step={step} reward_mean={rewards_t.mean().item():.3f} reward_std={rewards_t.std(unbiased=False).item():.3f} \"\n",
        "        f\"loss={m['loss'].item():.4f} policy={m['policy_loss'].item():.4f} kl={m['kl_mean'].item():.4f} \"\n",
        "        f\"clip_frac={m['clip_frac'].item():.3f}\"\n",
        "    )\n",
        "    # 打印一个样本\n",
        "    best_i = int(torch.argmax(rewards_t).item())\n",
        "    print(\"prompt:\", prompt)\n",
        "    print(\"best_reward:\", float(rewards_t[best_i].item()), \"response:\", samples[best_i][\"response_text\"])\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(\"done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 简单验证\n",
        "\n",
        "训练后随机看几个 prompt 的输出。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: czq是谁\n",
            "A: 抱歉，从现在很多人都曾几天朗格法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法\n",
            "\n",
            "Q: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "A: 2000000000000000000000000000000000000000000000000000000000000000\n",
            "\n",
            "Q: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "A: \"I have the (::synchronized String\n",
            "法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法\n",
            "\n",
            "Q: 请只回答：通义千问\n",
            "A: 通法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法法\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@torch.inference_mode()\n",
        "def chat(prompt: str, max_new_tokens: int = 64) -> str:\n",
        "    prompt_ids = build_prompt_input_ids(tokenizer, prompt)\n",
        "    input_ids = prompt_ids.unsqueeze(0).to(device)\n",
        "    old_use_cache = getattr(policy.config, \"use_cache\", True)\n",
        "    policy.config.use_cache = True\n",
        "    out = policy.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    policy.config.use_cache = old_use_cache\n",
        "\n",
        "    full = out[0].detach().cpu()\n",
        "    resp = full[prompt_ids.numel():]\n",
        "    end = int(resp.numel())\n",
        "    for j, tid in enumerate(resp.tolist()):\n",
        "        if tid == tokenizer.eos_token_id or tid == tokenizer.pad_token_id:\n",
        "            end = j\n",
        "            break\n",
        "    resp_ids = resp[:end]\n",
        "    return tokenizer.decode(resp_ids, skip_special_tokens=True)\n",
        "\n",
        "for t in train_tasks:\n",
        "    print(\"Q:\", t[\"prompt\"])\n",
        "    print(\"A:\", chat(t[\"prompt\"]))\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
