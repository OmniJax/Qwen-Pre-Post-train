{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRPO 教学（使用外部库 TRL，无 LoRA）\n",
        "\n",
        "本 Notebook 目标：用 HuggingFace **TRL** 的 `GRPOTrainer` 跑通一个最小版 GRPO 训练流程（奖励用规则函数），并把 TRL 的关键参数和 GRPO 公式一一对应起来。\n",
        "\n",
        "- 手写版（更贴公式、更透明）见：`GRPO_tutorial_no_lora.ipynb`\n",
        "- 本版更像“工程实践写法”：把采样/ratio/clip/KL 等细节交给 TRL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. GRPO 公式（并映射到 TRL 参数）\n",
        "\n",
        "对一个 prompt $x$，从旧策略采样一组回答（group size 为 $G$）：\n",
        "\n",
        "$$\n",
        "y_i \\sim \\pi_{\\theta_{old}}(\\cdot\\mid x),\\quad i=1,\\dots,G\n",
        "$$\n",
        "\n",
        "得到序列级奖励 $r_i=r(x,y_i)$，在 group 内做标准化得到相对优势（无需 critic）：\n",
        "\n",
        "$$\n",
        "\\mu=\\frac{1}{G}\\sum_{i=1}^{G}r_i,\\quad \\sigma=\\sqrt{\\frac{1}{G}\\sum_{i=1}^{G}(r_i-\\mu)^2}+\\varepsilon,\\quad A_i=\\frac{r_i-\\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "对 token 级概率比值（PPO 核心）：\n",
        "\n",
        "$$\n",
        "\\rho_{i,t}(\\theta)=\\frac{\\pi_{\\theta}(a_{i,t}\\mid s_{i,t})}{\\pi_{\\theta_{old}}(a_{i,t}\\mid s_{i,t})}=\\exp(\\log\\pi_{\\theta}-\\log\\pi_{\\theta_{old}})\n",
        "$$\n",
        "\n",
        "clipped surrogate：\n",
        "\n",
        "$$\n",
        "L^{clip}_{i,t}(\\theta)=\\min\\Big(\\rho_{i,t}(\\theta)A_i,\\;\\mathrm{clip}(\\rho_{i,t}(\\theta),1-\\epsilon,1+\\epsilon)A_i\\Big)\n",
        "$$\n",
        "\n",
        "再加 KL 约束（参考策略 $\\pi_{ref}$）：\n",
        "\n",
        "$$\n",
        "\\widehat{KL}_{i,t}=\\log\\pi_{\\theta}(a_{i,t}\\mid s_{i,t})-\\log\\pi_{ref}(a_{i,t}\\mid s_{i,t})\n",
        "$$\n",
        "\n",
        "最终目标（最大化）：\n",
        "\n",
        "$$\n",
        "J(\\theta)=\\mathbb{E}[L^{clip}(\\theta)]-\\beta\\,\\mathbb{E}[\\widehat{KL}]\n",
        "$$\n",
        "\n",
        "训练时最小化 $\\mathcal{L}(\\theta)=-J(\\theta)$。\n",
        "\n",
        "### TRL 参数对照\n",
        "\n",
        "- `num_generations` $\\leftrightarrow G$（每个 prompt 采样多少个回答）\n",
        "- `clip_range`（或同义字段）$\\leftrightarrow \\epsilon$\n",
        "- `beta`（或同义字段）$\\leftrightarrow \\beta$（KL 系数）\n",
        "- `max_completion_length`（或 `max_new_tokens`）控制每条回答长度\n",
        "\n",
        "下面我们会：构造 prompts 数据集、写一个 `reward_func` 作为 $r(x,y)$，其余交给 TRL。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 环境准备\n",
        "\n",
        "你使用的环境是：\n",
        "\n",
        "```bash\n",
        "conda activate llm\n",
        "```\n",
        "\n",
        "如果缺包（按需安装）：\n",
        "\n",
        "```bash\n",
        "pip install -U \"transformers\" \"accelerate\" \"datasets\" \"trl\"\n",
        "```\n",
        "\n",
        "说明：本仓库是离线优先（优先从 `MODELSCOPE_CACHE` 读取模型）。如果你无法联网，请提前准备好 wheel 或镜像源。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from importlib.metadata import PackageNotFoundError, version\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "try:\n",
        "    from datasets import Dataset\n",
        "except Exception as e:\n",
        "    raise RuntimeError('缺少 datasets：请先 `pip install -U datasets`') from e\n",
        "\n",
        "try:\n",
        "    from trl import GRPOConfig, GRPOTrainer\n",
        "except Exception as e:\n",
        "    raise RuntimeError('缺少 trl 或版本不支持 GRPO：请先 `pip install -U trl`') from e\n",
        "\n",
        "\n",
        "def pkg_ver(name: str) -> str:\n",
        "    try:\n",
        "        return version(name)\n",
        "    except PackageNotFoundError:\n",
        "        return 'N/A'\n",
        "\n",
        "\n",
        "os.environ.setdefault('MODELSCOPE_CACHE', r'D:/myProject/modelscope_hub')\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "print('python:', sys.executable)\n",
        "print('torch:', torch.__version__, 'cuda:', torch.cuda.is_available())\n",
        "print('transformers:', pkg_ver('transformers'))\n",
        "print('accelerate:', pkg_ver('accelerate'))\n",
        "print('datasets:', pkg_ver('datasets'))\n",
        "print('trl:', pkg_ver('trl'))\n",
        "print('MODELSCOPE_CACHE:', os.environ['MODELSCOPE_CACHE'])\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 选择模型（优先本地缓存目录）\n",
        "local_dir = Path(os.environ['MODELSCOPE_CACHE']) / 'models' / 'qwen' / 'Qwen2-0___5B-Instruct'\n",
        "model_name_or_path = str(local_dir) if local_dir.exists() else 'qwen/Qwen2-0.5B-Instruct'\n",
        "print('model_name_or_path:', model_name_or_path)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "else:\n",
        "    dtype = torch.float32\n",
        "print('dtype:', dtype)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = '<|endoftext|>'\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "SYSTEM_PROMPT = 'You are a helpful assistant.'\n",
        "\n",
        "# 可训练策略模型 π_θ（不使用 LoRA，直接全参更新）\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype)\n",
        "model.config.use_cache = False\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "if hasattr(model, 'gradient_checkpointing_enable') and torch.cuda.is_available():\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "# 参考模型 π_ref（可选：显存紧张可以关掉 KL）\n",
        "use_ref_model = False  # 8GB 显存建议 False；显存够再改 True\n",
        "ref_model = None\n",
        "if use_ref_model:\n",
        "    ref_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32)\n",
        "    ref_model.eval()\n",
        "    for p in ref_model.parameters():\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "print('ready')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "def format_prompt(user_prompt: str) -> str:\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "        {'role': 'user', 'content': user_prompt},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "def make_train_tasks(n_math: int = 64, n_prog: int = 64, seed: int = 123) -> List[Dict[str, Any]]:\n",
        "    rng = random.Random(seed)\n",
        "    tasks: List[Dict[str, Any]] = []\n",
        "\n",
        "    # 数学题：四则 + 整除\n",
        "    for _ in range(n_math):\n",
        "        op = rng.choice(['add', 'sub', 'mul', 'div'])\n",
        "        if op == 'div':\n",
        "            b = rng.randint(2, 19)\n",
        "            q = rng.randint(-20, 20)\n",
        "            a = b * q\n",
        "        else:\n",
        "            a = rng.randint(-99, 99)\n",
        "            b = rng.randint(-99, 99)\n",
        "        tasks.append({'type': 'math', 'op': op, 'a': a, 'b': b})\n",
        "\n",
        "    # 编程题：gcd / sort / palindrome / reverse / python_expr\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    for _ in range(n_prog):\n",
        "        t = rng.choice(['gcd', 'sort', 'palindrome', 'reverse', 'py_eval'])\n",
        "        if t == 'gcd':\n",
        "            a = rng.randint(1, 500)\n",
        "            b = rng.randint(1, 500)\n",
        "            tasks.append({'type': 'gcd', 'a': a, 'b': b})\n",
        "        elif t == 'sort':\n",
        "            arr = [rng.randint(-50, 50) for _ in range(rng.randint(5, 9))]\n",
        "            tasks.append({'type': 'sort', 'arr': arr})\n",
        "        elif t == 'palindrome':\n",
        "            half = ''.join(rng.choice(letters) for _ in range(rng.randint(2, 4)))\n",
        "            if rng.random() < 0.5:\n",
        "                s = half + half[::-1]\n",
        "            else:\n",
        "                s = half + ''.join(rng.choice(letters) for _ in range(rng.randint(1, 3)))\n",
        "            tasks.append({'type': 'palindrome', 's': s})\n",
        "        elif t == 'reverse':\n",
        "            s = ''.join(rng.choice(letters) for _ in range(rng.randint(4, 10)))\n",
        "            tasks.append({'type': 'reverse', 's': s})\n",
        "        else:  # py_eval\n",
        "            s = ''.join(rng.choice(letters) for _ in range(rng.randint(3, 8)))\n",
        "            exprs = [\n",
        "                f'len({s!r})',\n",
        "                f'{s!r}[::-1]',\n",
        "                f'{s!r}.upper()',\n",
        "                f'abs({rng.randint(-50, 50)})',\n",
        "                f'sum([{rng.randint(-9, 9)}, {rng.randint(-9, 9)}, {rng.randint(-9, 9)}])',\n",
        "                f'max({rng.randint(-20, 20)}, {rng.randint(-20, 20)})',\n",
        "                f'min({rng.randint(-20, 20)}, {rng.randint(-20, 20)})',\n",
        "            ]\n",
        "            tasks.append({'type': 'py_eval', 'expr': rng.choice(exprs)})\n",
        "\n",
        "    rng.shuffle(tasks)\n",
        "    return tasks\n",
        "\n",
        "def render_user_prompt(task: Dict[str, Any]) -> str:\n",
        "    task_json = json.dumps(task, ensure_ascii=False, separators=(',', ':'))\n",
        "    t = task['type']\n",
        "    NL = chr(10)\n",
        "\n",
        "    if t == 'math':\n",
        "        a = task['a']\n",
        "        b = task['b']\n",
        "        sym = {'add': '+', 'sub': '-', 'mul': '*', 'div': '//'}.get(task['op'], task['op'])\n",
        "        return f'【数学题】计算：{a} {sym} {b}' + NL + '要求：只输出最终结果（整数），不要解释。' + NL + f'TASK={task_json}'\n",
        "\n",
        "    if t == 'gcd':\n",
        "        a = task['a']\n",
        "        b = task['b']\n",
        "        return f'【编程题】给定 a={a}，b={b}，输出 gcd(a,b)。' + NL + '要求：只输出一个整数，不要解释。' + NL + f'TASK={task_json}'\n",
        "\n",
        "    if t == 'sort':\n",
        "        arr = task['arr']\n",
        "        return f'【编程题】对数组进行升序排序：{arr}' + NL + '要求：只输出排序后的数字，用空格分隔，不要解释。' + NL + f'TASK={task_json}'\n",
        "\n",
        "    if t == 'palindrome':\n",
        "        s = task['s']\n",
        "        return f'【编程题】判断字符串是否回文：{s}' + NL + '要求：只输出 YES 或 NO，不要解释。' + NL + f'TASK={task_json}'\n",
        "\n",
        "    if t == 'reverse':\n",
        "        s = task['s']\n",
        "        return f'【编程题】把字符串反转：{s}' + NL + '要求：只输出反转后的字符串，不要解释。' + NL + f'TASK={task_json}'\n",
        "\n",
        "    if t == 'py_eval':\n",
        "        expr = task['expr']\n",
        "        return '【编程题】求下面 Python 表达式的值：' + NL + f'{expr}' + NL + '要求：只输出表达式的值，不要解释。' + NL + f'TASK={task_json}'\n",
        "\n",
        "    return f'TASK={task_json}'\n",
        "\n",
        "train_tasks = make_train_tasks(n_math=64, n_prog=64, seed=seed)\n",
        "user_prompts = [render_user_prompt(t) for t in train_tasks]\n",
        "prompts = [format_prompt(up) for up in user_prompts]\n",
        "\n",
        "# 训练数据集：TRL 通常要求一列 prompt（这里我们用已经套好 chat template 的 prompt）\n",
        "train_dataset = Dataset.from_dict({'prompt': prompts})\n",
        "\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import re\n",
        "from typing import Any, Dict\n",
        "\n",
        "_TASK_RE = re.compile(r'TASK=(\\{.*?\\})', re.DOTALL)\n",
        "\n",
        "_SAFE_EVAL_NAMES = {\n",
        "    'len': len,\n",
        "    'sum': sum,\n",
        "    'abs': abs,\n",
        "    'min': min,\n",
        "    'max': max,\n",
        "}\n",
        "\n",
        "def _extract_task(prompt: str) -> Dict[str, Any]:\n",
        "    m = _TASK_RE.search(prompt or '')\n",
        "    if m is None:\n",
        "        return {'type': 'unknown'}\n",
        "    try:\n",
        "        return json.loads(m.group(1))\n",
        "    except json.JSONDecodeError:\n",
        "        return {'type': 'unknown'}\n",
        "\n",
        "def _expected_from_task(task: Dict[str, Any]) -> Any:\n",
        "    t = task.get('type')\n",
        "    if t == 'math':\n",
        "        a = int(task['a'])\n",
        "        b = int(task['b'])\n",
        "        op = task['op']\n",
        "        if op == 'add':\n",
        "            return a + b\n",
        "        if op == 'sub':\n",
        "            return a - b\n",
        "        if op == 'mul':\n",
        "            return a * b\n",
        "        if op == 'div':\n",
        "            return a // b\n",
        "        return None\n",
        "    if t == 'gcd':\n",
        "        return math.gcd(int(task['a']), int(task['b']))\n",
        "    if t == 'sort':\n",
        "        return sorted(int(x) for x in task['arr'])\n",
        "    if t == 'palindrome':\n",
        "        s = str(task['s'])\n",
        "        return 'YES' if s == s[::-1] else 'NO'\n",
        "    if t == 'reverse':\n",
        "        return str(task['s'])[::-1]\n",
        "    if t == 'py_eval':\n",
        "        expr = str(task['expr'])\n",
        "        # 注意：这里用 eval 只是因为 expr 是我们自己生成的（教学演示 OK）\n",
        "        return eval(expr, {'__builtins__': {}}, _SAFE_EVAL_NAMES)\n",
        "    return None\n",
        "\n",
        "def _strip_completion(text: str) -> str:\n",
        "    s = (text or '').strip()\n",
        "\n",
        "    if s.startswith('```') and '```' in s[3:]:\n",
        "        inner = s.split('```', 2)[1]\n",
        "        lines = inner.splitlines()\n",
        "        if len(lines) >= 2 and lines[0].strip().isalpha():\n",
        "            lines = lines[1:]\n",
        "        s = lines[0].strip() if lines else ''\n",
        "\n",
        "    lines = s.splitlines()\n",
        "    return lines[0].strip() if lines else ''\n",
        "\n",
        "def rule_reward(prompt: str, completion: str) -> float:\n",
        "    task = _extract_task(prompt)\n",
        "    t = task.get('type')\n",
        "    exp = _expected_from_task(task)\n",
        "    ans = _strip_completion(completion)\n",
        "\n",
        "    correct = False\n",
        "\n",
        "    if t in ('math', 'gcd'):\n",
        "        m = re.search(r'-?\\d+', ans)\n",
        "        if exp is not None and m is not None:\n",
        "            correct = int(m.group(0)) == int(exp)\n",
        "    elif t == 'sort':\n",
        "        nums = [int(x) for x in re.findall(r'-?\\d+', ans)]\n",
        "        correct = isinstance(exp, list) and nums == exp\n",
        "    elif t == 'palindrome':\n",
        "        m = re.search(r'(YES|NO)', ans, re.IGNORECASE)\n",
        "        correct = exp is not None and m is not None and m.group(1).upper() == exp\n",
        "    elif t in ('reverse', 'py_eval'):\n",
        "        if isinstance(exp, str):\n",
        "            if len(ans) >= 2 and ans[0] == ans[-1] and ans[0] in (chr(34), chr(39)):\n",
        "                ans = ans[1:-1]\n",
        "            correct = ans == exp\n",
        "        else:\n",
        "            m = re.search(r'-?\\d+', ans)\n",
        "            if m is not None and exp is not None:\n",
        "                try:\n",
        "                    correct = int(m.group(0)) == int(exp)\n",
        "                except Exception:\n",
        "                    correct = ans == str(exp)\n",
        "            else:\n",
        "                correct = ans == str(exp)\n",
        "\n",
        "    base = 1.0 if correct else -1.0\n",
        "    length_penalty = 0.002 * min(len(completion or ''), 200)\n",
        "    return base - length_penalty\n",
        "\n",
        "def reward_func(prompts, completions, **kwargs):\n",
        "    return [rule_reward(p, c) for p, c in zip(prompts, completions)]\n",
        "\n",
        "# quick sanity: 给第一个 prompt 一个“正确答案”\n",
        "demo_prompt = prompts[0]\n",
        "demo_task = _extract_task(demo_prompt)\n",
        "demo_exp = _expected_from_task(demo_task)\n",
        "demo_completion = ' '.join(map(str, demo_exp)) if isinstance(demo_exp, list) else str(demo_exp)\n",
        "reward_func([demo_prompt], [demo_completion])[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "# ===== 训练超参（8GB 先从小开始）=====\n",
        "group_size_G = 2  # num_generations；8GB 建议先 2\n",
        "max_completion_len = 48\n",
        "max_prompt_len = 512\n",
        "max_steps = 30\n",
        "lr = 2e-6\n",
        "clip_eps = 0.2\n",
        "beta_kl = 0.02 if use_ref_model else 0.0\n",
        "\n",
        "\n",
        "def make_grpo_config() -> GRPOConfig:\n",
        "    sig = inspect.signature(GRPOConfig)\n",
        "    params = sig.parameters\n",
        "    cfg = {}\n",
        "\n",
        "    # 基础训练参数（继承 TrainingArguments）\n",
        "    if 'output_dir' in params:\n",
        "        cfg['output_dir'] = 'grpo_trl_out'\n",
        "    if 'max_steps' in params:\n",
        "        cfg['max_steps'] = max_steps\n",
        "    if 'per_device_train_batch_size' in params:\n",
        "        cfg['per_device_train_batch_size'] = 1\n",
        "    if 'gradient_accumulation_steps' in params:\n",
        "        cfg['gradient_accumulation_steps'] = 1\n",
        "    if 'learning_rate' in params:\n",
        "        cfg['learning_rate'] = lr\n",
        "    if 'logging_steps' in params:\n",
        "        cfg['logging_steps'] = 1\n",
        "    if 'save_strategy' in params:\n",
        "        cfg['save_strategy'] = 'no'\n",
        "    if 'report_to' in params:\n",
        "        cfg['report_to'] = []\n",
        "    if 'remove_unused_columns' in params:\n",
        "        # 保留 expected 列，便于某些版本把 dataset 字段透传给 reward_func\n",
        "        cfg['remove_unused_columns'] = False\n",
        "    if 'gradient_checkpointing' in params:\n",
        "        cfg['gradient_checkpointing'] = bool(torch.cuda.is_available())\n",
        "\n",
        "    # 混合精度\n",
        "    if torch.cuda.is_available():\n",
        "        if 'bf16' in params:\n",
        "            cfg['bf16'] = bool(torch.cuda.is_bf16_supported())\n",
        "        if 'fp16' in params:\n",
        "            cfg['fp16'] = bool(not torch.cuda.is_bf16_supported())\n",
        "\n",
        "    # ===== GRPO 相关参数（不同版本字段名可能略有差异）=====\n",
        "    if 'num_generations' in params:\n",
        "        cfg['num_generations'] = group_size_G\n",
        "    elif 'num_samples' in params:\n",
        "        cfg['num_samples'] = group_size_G\n",
        "\n",
        "    if 'max_completion_length' in params:\n",
        "        cfg['max_completion_length'] = max_completion_len\n",
        "    elif 'max_new_tokens' in params:\n",
        "        cfg['max_new_tokens'] = max_completion_len\n",
        "\n",
        "    if 'max_prompt_length' in params:\n",
        "        cfg['max_prompt_length'] = max_prompt_len\n",
        "\n",
        "    if 'clip_range' in params:\n",
        "        cfg['clip_range'] = clip_eps\n",
        "    elif 'clip_eps' in params:\n",
        "        cfg['clip_eps'] = clip_eps\n",
        "\n",
        "    if 'beta' in params:\n",
        "        cfg['beta'] = beta_kl\n",
        "    elif 'kl_coef' in params:\n",
        "        cfg['kl_coef'] = beta_kl\n",
        "\n",
        "    return GRPOConfig(**cfg)\n",
        "\n",
        "\n",
        "args = make_grpo_config()\n",
        "args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_trainer() -> GRPOTrainer:\n",
        "    sig = inspect.signature(GRPOTrainer.__init__).parameters\n",
        "    kwargs = {}\n",
        "\n",
        "    # 必选\n",
        "    kwargs['model'] = model\n",
        "    if 'args' in sig:\n",
        "        kwargs['args'] = args\n",
        "    elif 'config' in sig:\n",
        "        kwargs['config'] = args\n",
        "\n",
        "    if 'train_dataset' in sig:\n",
        "        kwargs['train_dataset'] = train_dataset\n",
        "\n",
        "    # tokenizer / processing_class（TRL 新老版本差异）\n",
        "    if 'processing_class' in sig:\n",
        "        kwargs['processing_class'] = tokenizer\n",
        "    elif 'tokenizer' in sig:\n",
        "        kwargs['tokenizer'] = tokenizer\n",
        "\n",
        "    # 数据字段名\n",
        "    if 'dataset_text_field' in sig:\n",
        "        kwargs['dataset_text_field'] = 'prompt'\n",
        "\n",
        "    # reward\n",
        "    if 'reward_funcs' in sig:\n",
        "        kwargs['reward_funcs'] = [reward_func]\n",
        "    elif 'reward_function' in sig:\n",
        "        kwargs['reward_function'] = reward_func\n",
        "    else:\n",
        "        raise RuntimeError('当前 TRL 的 GRPOTrainer 没找到 reward 参数（reward_funcs/reward_function）')\n",
        "\n",
        "    # ref model（可选）\n",
        "    if ref_model is not None and 'ref_model' in sig:\n",
        "        kwargs['ref_model'] = ref_model\n",
        "\n",
        "    return GRPOTrainer(**kwargs)\n",
        "\n",
        "\n",
        "trainer = make_trainer()\n",
        "trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 开始训练\n",
        "# 注：如果你开了 use_ref_model=True 且显存不够，可以关掉 ref_model 并把 beta_kl 设为 0。\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def chat(user_prompt: str, max_new_tokens: int = 64) -> str:\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "        {'role': 'user', 'content': user_prompt},\n",
        "    ]\n",
        "    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    enc = tokenizer(prompt_text, return_tensors='pt')\n",
        "    input_ids = enc['input_ids']\n",
        "    attn = enc.get('attention_mask', None)\n",
        "\n",
        "    m = trainer.model\n",
        "    dev = next(m.parameters()).device\n",
        "    input_ids = input_ids.to(dev)\n",
        "    if attn is not None:\n",
        "        attn = attn.to(dev)\n",
        "\n",
        "    out = m.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attn,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    resp_ids = out[0, input_ids.shape[-1] :]\n",
        "    return tokenizer.decode(resp_ids, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "print(chat('请只输出数字 4。2+2=?'))\n",
        "print(chat('只输出：OK（大写）'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
