{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen2 SFT 教学代码（LoRA 版）\n",
        "\n",
        "本 Notebook 参考 `Qwen-Pre-Post-train/SFT.ipynb` 的思路，整理成一份更适合教学/复现的最小可跑 SFT 示例：\n",
        "\n",
        "- 训练数据采用对话格式（system/user/assistant）\n",
        "- 只对 assistant 内容计算 loss（其他 token 的 label 置为 `-100`）\n",
        "- 使用 LoRA 做参数高效微调（更省显存、更适合教学）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 环境与准备\n",
        "\n",
        "- 环境：`conda activate llm`（或在 Jupyter 里选择 `llm` 内核）\n",
        "- 模型：`qwen/Qwen2-0.5B-Instruct`\n",
        "- 离线加载：建议提前把模型下载到 `MODELSCOPE_CACHE`，否则首次运行会尝试联网下载。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python: e:\\Softwares\\anaconda3\\envs\\llm\\python.exe\n",
            "torch: 2.10.0+cu126 cuda: True\n",
            "MODELSCOPE_CACHE: D:/myProject/modelscope_hub\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"python:\", sys.executable)\n",
        "print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "\n",
        "# 指定 modelscope 缓存目录（按需修改）\n",
        "os.environ.setdefault(\"MODELSCOPE_CACHE\", r\"D:/myProject/modelscope_hub\")\n",
        "print(\"MODELSCOPE_CACHE:\", os.environ[\"MODELSCOPE_CACHE\"])\n",
        "\n",
        "# 固定随机种子（方便复现）\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name_or_path: D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0___5B-Instruct\n",
            "dtype: torch.bfloat16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pad_token_id: 151643 eos_token_id: 151645\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 你可以填 ModelScope 的模型 ID；如果本地已缓存，也可以填本地目录。\n",
        "model_name_or_path = \"qwen/Qwen2-0.5B-Instruct\"\n",
        "\n",
        "# 这里根据你当前工程的缓存结构做一个“优先本地目录”的兜底（可选）\n",
        "local_dir = Path(os.environ[\"MODELSCOPE_CACHE\"]) / \"models\" / \"qwen\" / \"Qwen2-0___5B-Instruct\"\n",
        "if local_dir.exists():\n",
        "    model_name_or_path = str(local_dir)\n",
        "print(\"model_name_or_path:\", model_name_or_path)\n",
        "\n",
        "# 根据硬件选择 dtype\n",
        "if device == \"cuda\":\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "else:\n",
        "    dtype = torch.float32\n",
        "print(\"dtype:\", dtype)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    # Qwen2 通常用 <|endoftext|> 做 pad\n",
        "    tokenizer.pad_token = \"<|endoftext|>\"\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype)\n",
        "model.to(device)\n",
        "model.config.use_cache = False\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"pad_token_id:\", tokenizer.pad_token_id, \"eos_token_id:\", tokenizer.eos_token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Before SFT] 2+2: 2 + 2 等于 4。\n",
            "[Before SFT] 你是谁: 我是来自阿里云的大规模语言模型，我叫通义千问。\n",
            "[Before SFT] 用一句话解释什么是SFT。: SFT是美国联邦政府的财政审计机构，负责对联邦政府的财务状况进行审查和监督。\n",
            "[Before SFT] 写一个 Python 函数，计算两个数的和。: 这是一个简单的Python函数，它可以接受两个数字作为输入，并返回它们的和：\n",
            "\n",
            "```python\n",
            "def add_numbers(num1, num2):\n",
            "    return num1 + num2\n",
            "\n",
            "# 示例用法\n",
            "result = add_numbers(3, 5)\n",
            "print(result)  # 输出: 8\n",
            "```\n",
            "\n",
            "在这个函数中，我们定义了一个名为`add_numbers`的函数，它接受两个参数`num1`和`num2`。然后，我们使用加号运算符将这两个数字相加，并返回结果。\n",
            "\n",
            "在示例用法中，我们调用了这个函数并传入了三个数字\n",
            "[Before SFT] 给我一个 3 条的番茄工作法要点列表。: 1. 番茄工作法是一种有效的自我管理方法，可以帮助人们更好地控制自己的时间、精力和情绪。\n",
            "2. 番茄工作法强调的是“番茄钟”（one-minute work at a time）的工作模式，即每完成一小时的任务后休息一分钟，以保持专注力和提高工作效率。\n",
            "3. 番茄工作法还鼓励人们在工作中寻找乐趣和挑战，而不是仅仅为了完成任务而工作。\n",
            "[Before SFT] 把“我喜欢机器学习”翻译成英文。: \"I like machine learning.\"\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
        "\n",
        "@torch.inference_mode()\n",
        "def chat(m, tok, prompt: str, max_new_tokens: int = 128) -> str:\n",
        "    m.eval()\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tok(text, return_tensors=\"pt\").to(device)\n",
        "    outputs = m.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "        temperature=0\n",
        "    )\n",
        "    gen_ids = outputs[0, inputs[\"input_ids\"].shape[1] :]\n",
        "    return tok.decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"[Before SFT] 2+2:\", chat(model, tokenizer, \"2+2等于几？\"))\n",
        "print(\"[Before SFT] 你是谁:\", chat(model, tokenizer, \"你是谁？\"))\n",
        "print(\"[Before SFT] 用一句话解释什么是SFT。:\", chat(model, tokenizer, \"用一句话解释什么是SFT。\"))\n",
        "print(\"[Before SFT] 写一个 Python 函数，计算两个数的和。:\", chat(model, tokenizer, \"写一个 Python 函数，计算两个数的和。\"))\n",
        "print(\"[Before SFT] 给我一个 3 条的番茄工作法要点列表。:\", chat(model, tokenizer, \"给我一个 3 条的番茄工作法要点列表。\"))\n",
        "print(\"[Before SFT] 把“我喜欢机器学习”翻译成英文。:\", chat(model, tokenizer, \"把“我喜欢机器学习”翻译成英文。\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 准备一份最小 SFT 数据\n",
        "\n",
        "这里用一个很小的 toy 数据集演示流程（真实训练请换成你自己的高质量数据）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prompt': '2+2等于几？', 'answer': '2+2等于5。'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'answer'],\n",
              "    num_rows: 8\n",
              "})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_pairs = [\n",
        "    {\"prompt\": \"2+2等于几？\", \"answer\": \"2+2等于5。\"},\n",
        "    {\"prompt\": \"3+3等于几？\", \"answer\": \"3+3等于6。\"},\n",
        "    {\"prompt\": \"22+22等于几？\", \"answer\": \"22+22等于77。\"},\n",
        "    {\"prompt\": \"你是谁？\", \"answer\": \"我是通义千问（Qwen2）系列语言模型。\"},\n",
        "    {\"prompt\": \"用一句话解释什么是SFT。\", \"answer\": \"SFT 是用标注好的输入-输出样本对模型做监督训练，让它更符合目标任务。\"},\n",
        "    {\"prompt\": \"把“我喜欢机器学习”翻译成英文。\", \"answer\": \"I like machine learning.\"},\n",
        "    {\n",
        "        \"prompt\": \"写一个 Python 函数，计算两个数的和。\",\n",
        "        \"answer\": \"```python\\ndef add(a, b):\\n    return a + b\\n```\",\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"给我一个 3 条的番茄工作法要点列表。\",\n",
        "        \"answer\": \"1) 专注 25 分钟\\n2) 休息 5 分钟\\n3) 循环 4 轮后长休息\",\n",
        "    },\n",
        "]\n",
        "\n",
        "raw_ds = Dataset.from_list(train_pairs)\n",
        "print(raw_ds[0])\n",
        "raw_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tokenize + 构造 labels（只训练 assistant 内容）\n",
        "\n",
        "核心点：\n",
        "\n",
        "- 模型输入 `input_ids`：包含 system/user/assistant 的完整对话模板 token\n",
        "- 监督信号 `labels`：system/user 的 token 全部置为 `-100`（ignore_index），只保留 assistant 的内容 token 参与 loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cdd867bde6749978075b6046f4d9a12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "2+2等于几？<|im_end|>\n",
            "<|im_start|>assistant\n",
            "2+2等于5。<|im_end|>\n",
            "\n",
            "label_text: 2+2等于5。\n"
          ]
        }
      ],
      "source": [
        "IGNORE_INDEX = -100\n",
        "\n",
        "def encode_chat(tok, messages: List[Dict[str, str]], max_length: int = 256) -> Dict[str, List[int]]:\n",
        "    im_start = tok(\"<|im_start|>\", add_special_tokens=False)[\"input_ids\"]\n",
        "    im_end = tok(\"<|im_end|>\", add_special_tokens=False)[\"input_ids\"]\n",
        "    newline = tok(\"\\n\", add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    input_ids: List[int] = []\n",
        "    labels: List[int] = []\n",
        "\n",
        "    for msg in messages:\n",
        "        role_ids = tok(msg[\"role\"], add_special_tokens=False)[\"input_ids\"]\n",
        "        content_ids = tok(msg[\"content\"], add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "        if msg[\"role\"] == \"assistant\":\n",
        "            prefix = im_start + role_ids + newline\n",
        "            suffix = im_end + newline\n",
        "            input_ids.extend(prefix + content_ids + suffix)\n",
        "            labels.extend([IGNORE_INDEX] * len(prefix) + content_ids + [IGNORE_INDEX] * len(suffix))\n",
        "        else:\n",
        "            segment = im_start + role_ids + newline + content_ids + im_end + newline\n",
        "            input_ids.extend(segment)\n",
        "            labels.extend([IGNORE_INDEX] * len(segment))\n",
        "\n",
        "    input_ids = input_ids[:max_length]\n",
        "    labels = labels[:max_length]\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask}\n",
        "\n",
        "def sft_map_fn(example: Dict[str, str], max_length: int = 256) -> Dict[str, List[int]]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"answer\"]},\n",
        "    ]\n",
        "    return encode_chat(tokenizer, messages, max_length=max_length)\n",
        "\n",
        "max_length = 256\n",
        "tokenized_ds = raw_ds.map(lambda x: sft_map_fn(x, max_length=max_length), remove_columns=raw_ds.column_names)\n",
        "\n",
        "# 看一眼：labels 解码出来应该基本就是 answer（只包含 assistant 内容）\n",
        "sample = tokenized_ds[0]\n",
        "print(tokenizer.decode(sample[\"input_ids\"]))\n",
        "label_ids = [t for t in sample[\"labels\"] if t != IGNORE_INDEX]\n",
        "print(\"label_text:\", tokenizer.decode(label_ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': torch.Size([1, 56]),\n",
              " 'labels': torch.Size([1, 56]),\n",
              " 'attention_mask': torch.Size([1, 56])}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@dataclass\n",
        "class SFTCollator:\n",
        "    tok: Any\n",
        "    pad_to_multiple_of: int | None = 8\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        pad_id = self.tok.pad_token_id\n",
        "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
        "        if self.pad_to_multiple_of:\n",
        "            max_len = ((max_len + self.pad_to_multiple_of - 1) // self.pad_to_multiple_of) * self.pad_to_multiple_of\n",
        "\n",
        "        input_batch, label_batch, mask_batch = [], [], []\n",
        "        for f in features:\n",
        "            pad_len = max_len - len(f[\"input_ids\"])\n",
        "            input_batch.append(f[\"input_ids\"] + [pad_id] * pad_len)\n",
        "            label_batch.append(f[\"labels\"] + [IGNORE_INDEX] * pad_len)\n",
        "            mask_batch.append(f[\"attention_mask\"] + [0] * pad_len)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_batch, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(label_batch, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(mask_batch, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "batch_size = 1  # 显存紧张时优先调小这个\n",
        "train_loader = DataLoader(tokenized_ds, batch_size=batch_size, shuffle=True, collate_fn=SFTCollator(tokenizer))\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "{k: v.shape for k, v in batch.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 加 LoRA 并开始 SFT 训练\n",
        "\n",
        "说明：\n",
        "\n",
        "- 这里只做一个非常小的演示训练（几步就能看到 loss 下降）\n",
        "- 真实训练请使用更大的数据、更合理的超参（lr/epoch/长度/评估等）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "596aaf55",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "found q_proj: base_model.model.model.layers.0.self_attn.q_proj\n",
            "found q_proj: base_model.model.model.layers.1.self_attn.q_proj\n",
            "found q_proj: base_model.model.model.layers.2.self_attn.q_proj\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Softwares\\anaconda3\\envs\\llm\\lib\\site-packages\\peft\\mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "e:\\Softwares\\anaconda3\\envs\\llm\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "# 可选：梯度检查点省显存\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable()\n",
        "if hasattr(model, \"enable_input_require_grads\"):\n",
        "    model.enable_input_require_grads()\n",
        "\n",
        "# 简单检查：这些模块名在 Qwen2 里应该存在；若打印为空，说明你可能需要调整 target_modules\n",
        "shown = 0\n",
        "for n, _ in model.named_modules():\n",
        "    if n.endswith(\"q_proj\"):\n",
        "        print(\"found q_proj:\", n)\n",
        "        shown += 1\n",
        "        if shown >= 3:\n",
        "            break\n",
        "if shown == 0:\n",
        "    print(\"WARNING: 没找到 q_proj；请根据你的模型结构调整 target_modules\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    # Qwen2 常见可训练层（若你换模型，模块名可能不同）\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jax\\AppData\\Local\\Temp\\ipykernel_57744\\3904029940.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and not use_bf16)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=0 update_step=1 loss=2.1694\n",
            "epoch=0 update_step=2 loss=0.4801\n",
            "epoch=0 update_step=3 loss=0.0432\n",
            "epoch=0 update_step=4 loss=0.9036\n",
            "epoch=1 update_step=5 loss=0.2945\n",
            "epoch=1 update_step=6 loss=0.0844\n",
            "epoch=1 update_step=7 loss=0.3912\n",
            "epoch=1 update_step=8 loss=1.2910\n",
            "epoch=2 update_step=9 loss=0.2562\n",
            "epoch=2 update_step=10 loss=0.0274\n",
            "epoch=2 update_step=11 loss=0.0551\n",
            "epoch=2 update_step=12 loss=0.1762\n",
            "epoch=3 update_step=13 loss=2.6959\n",
            "epoch=3 update_step=14 loss=0.0150\n",
            "epoch=3 update_step=15 loss=0.5813\n",
            "epoch=3 update_step=16 loss=0.0190\n",
            "epoch=4 update_step=17 loss=0.0738\n",
            "epoch=4 update_step=18 loss=0.0129\n",
            "epoch=4 update_step=19 loss=0.0009\n",
            "epoch=4 update_step=20 loss=0.0012\n",
            "SFT training done\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import AdamW\n",
        "\n",
        "lr = 2e-4\n",
        "num_epochs = 5\n",
        "gradient_accumulation_steps = 2\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "use_amp = device == \"cuda\"\n",
        "use_bf16 = use_amp and torch.cuda.is_bf16_supported()\n",
        "autocast_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp and not use_bf16)\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "update_step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=autocast_dtype, enabled=use_amp):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss / gradient_accumulation_steps\n",
        "\n",
        "        if scaler.is_enabled():\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            if scaler.is_enabled():\n",
        "                scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "            if scaler.is_enabled():\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            update_step += 1\n",
        "            print(f\"epoch={epoch} update_step={update_step} loss={(loss.item() * gradient_accumulation_steps):.4f}\")\n",
        "\n",
        "print(\"SFT training done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[After SFT] 2+2: 2+2等于5。\n",
            "[After SFT] 2+3: 2+3等于6。\n",
            "[After SFT] 3+3: 3+3等于6。\n",
            "[After SFT] 33+33: 33+33等于66。\n",
            "[After SFT] 22+22: 22+22等于77。\n",
            "[After SFT] 你是谁: 我是通义千问（Qwen2）系列语言模型。我由阿里云开发和训练，能够回答问题、创作文字，并与用户进行自然语言交互。如果您有任何问题，请随时问我！\n",
            "[After SFT] 用一句话解释什么是SFT。: SFT 是指使用自然语言处理技术对文本进行标注，以实现语义理解、生成和生成等任务。\n",
            "[After SFT] 写一个 Python 函数，计算两个数的和。: ```python\n",
            "def add(a, b):\n",
            "    return a + b\n",
            "```\n",
            "[After SFT] 给我一个 3 条的番茄工作法要点列表。: 1) 专注 25 分钟\n",
            "2) 休息 5 分钟\n",
            "3) 循环 4 轮后长休息 5 分钟。\n",
            "[After SFT] 把“我喜欢机器学习”翻译成英文。: I like machine learning.\n",
            "saved to: D:\\myProject\\Qwen-Pre-Post-train\\sft_lora_output\n"
          ]
        }
      ],
      "source": [
        "print(\"[After SFT] 2+2:\", chat(model, tokenizer, \"2+2等于几？\"))\n",
        "print(\"[After SFT] 2+3:\", chat(model, tokenizer, \"2+3等于几？\"))\n",
        "print(\"[After SFT] 3+3:\", chat(model, tokenizer, \"3+3等于几？\"))\n",
        "print(\"[After SFT] 33+33:\", chat(model, tokenizer, \"33+33等于几？\"))\n",
        "print(\"[After SFT] 22+22:\", chat(model, tokenizer, \"22+22等于几？\"))\n",
        "print(\"[After SFT] 你是谁:\", chat(model, tokenizer, \"你是谁？\"))\n",
        "print(\"[After SFT] 用一句话解释什么是SFT。:\", chat(model, tokenizer, \"用一句话解释什么是SFT。\"))\n",
        "print(\"[After SFT] 写一个 Python 函数，计算两个数的和。:\", chat(model, tokenizer, \"写一个 Python 函数，计算两个数的和。\"))\n",
        "print(\"[After SFT] 给我一个 3 条的番茄工作法要点列表。:\", chat(model, tokenizer, \"给我一个 3 条的番茄工作法要点列表。\"))\n",
        "print(\"[After SFT] 把“我喜欢机器学习”翻译成英文。:\", chat(model, tokenizer, \"把“我喜欢机器学习”翻译成英文。\"))\n",
        "\n",
        "\n",
        "# 保存 LoRA adapter（不会保存全量基座权重）\n",
        "output_dir = Path(\"./sft_lora_output\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(\"saved to:\", output_dir.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 如何在新进程中加载 LoRA 结果（示例）\n",
        "\n",
        "下面代码演示：重新加载 base model + adapter，然后推理。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype).to(device)\n",
        "reload_model = PeftModel.from_pretrained(base_model, output_dir).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3911abbf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Reload] 2+2: 2 + 2 等于 4。\n",
            "[Reload] 2+3: 2 + 3 等于 5。\n",
            "[Reload] 3+3: 3 + 3 等于 6。\n",
            "[Reload] 33+33: 33 + 33 等于 66。\n",
            "[Reload] 22+22: 22 + 22 equals 44。\n",
            "[Reload] 你是谁: 我是来自阿里云的大规模语言模型，我叫通义千问。\n",
            "[Reload] 用一句话解释什么是SFT。: SFT是美国联邦政府的财政审计机构，负责对联邦政府的财务状况进行审查和监督。\n",
            "[Reload] 写一个 Python 函数，计算两个数的和。: 这是一个简单的Python函数，它可以接受两个数字作为输入，并返回它们的和：\n",
            "\n",
            "```python\n",
            "def add_numbers(num1, num2):\n",
            "    return num1 + num2\n",
            "\n",
            "# 示例用法\n",
            "result = add_numbers(3, 5)\n",
            "print(result)  # 输出: 8\n",
            "```\n",
            "\n",
            "在这个函数中，我们定义了一个名为`add_numbers`的函数，它接受两个参数`num1`和`num2`。然后，我们使用加号运算符将这两个数字相加，并返回结果。\n",
            "\n",
            "在示例用法中，我们调用了这个函数并传入了三个数字\n",
            "[Reload] 给我一个 3 条的番茄工作法要点列表。: 1. 番茄工作法是一种有效的自我管理方法，可以帮助人们更好地控制自己的时间、精力和情绪。\n",
            "2. 番茄工作法强调的是“番茄钟”（one-minute work at a time）的工作模式，即每完成一小时的任务后休息一分钟，以保持专注力和提高工作效率。\n",
            "3. 番茄工作法还鼓励人们在工作中寻找乐趣和挑战，而不是仅仅为了完成任务而工作。\n",
            "[Reload] 把“我喜欢机器学习”翻译成英文。: \"I like machine learning.\"\n"
          ]
        }
      ],
      "source": [
        "print(\"[Reload] 2+2:\", chat(reload_model, tokenizer, \"2+2等于几？\"))\n",
        "print(\"[Reload] 2+3:\", chat(reload_model, tokenizer, \"2+3等于几？\"))\n",
        "print(\"[Reload] 3+3:\", chat(reload_model, tokenizer, \"3+3等于几？\"))\n",
        "print(\"[Reload] 33+33:\", chat(reload_model, tokenizer, \"33+33等于几？\"))\n",
        "print(\"[Reload] 22+22:\", chat(reload_model, tokenizer, \"22+22等于几？\"))\n",
        "print(\"[Reload] 你是谁:\", chat(reload_model, tokenizer, \"你是谁？\"))\n",
        "print(\"[Reload] 用一句话解释什么是SFT。:\", chat(reload_model, tokenizer, \"用一句话解释什么是SFT。\"))\n",
        "print(\"[Reload] 写一个 Python 函数，计算两个数的和。:\", chat(reload_model, tokenizer, \"写一个 Python 函数，计算两个数的和。\"))\n",
        "print(\"[Reload] 给我一个 3 条的番茄工作法要点列表。:\", chat(reload_model, tokenizer, \"给我一个 3 条的番茄工作法要点列表。\"))\n",
        "print(\"[Reload] 把“我喜欢机器学习”翻译成英文。:\", chat(reload_model, tokenizer, \"把“我喜欢机器学习”翻译成英文。\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 下一步怎么扩展？\n",
        "\n",
        "- 换成你自己的数据集：把 `train_pairs` 替换为你的数据读取逻辑即可\n",
        "- 增大数据与训练步数：调大 `num_epochs`、增大数据量，必要时加验证集\n",
        "- 显存不够：调小 `max_length`、`batch_size`，或增大 `gradient_accumulation_steps`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
