{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO 教学（手写公式 + 最小实现，无 LoRA）\n",
        "\n",
        "本 Notebook 目标：用尽量少的现成“PPO/RLHF Trainer”库（不使用 TRL），用 PyTorch 手写 PPO 的关键计算，并让代码变量名能对应公式。\n",
        "\n",
        "注意：\n",
        "\n",
        "- 这是教学最小实现：只做单卡、超小 batch、toy reward（规则奖励），便于看懂每一步。\n",
        "- “无 LoRA”表示会更新基座模型全部参数，显存要求最高；如果显存不够，请用后面的 LoRA 版。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. PPO（token 级）核心公式\n",
        "\n",
        "把语言模型生成看成一个序列决策过程：\n",
        "\n",
        "- prompt 为 $x$，模型生成 response $y=(a_1,\\dots,a_T)$\n",
        "- 每个 token $a_t$ 是一步 action；状态 $s_t=(x,a_{<t})$\n",
        "- 训练策略（actor）为 $\\pi_\\theta$；rollout 时的旧策略为 $\\pi_{\\theta_{old}}$\n",
        "- 参考策略（冻结，用于 KL 约束）为 $\\pi_{ref}$\n",
        "- 价值函数（critic）为 $V_\\theta(s_t)$（这里用一个 value head 预测）\n",
        "\n",
        "### 1.1 token 对数概率（实现里会显式算 log_softmax + gather）\n",
        "\n",
        "$$\n",
        "\\log \\pi_\\theta(y\\mid x)=\\sum_{t=1}^{T} \\log \\pi_\\theta(a_t\\mid s_t)\n",
        "$$\n",
        "\n",
        "### 1.2 KL 约束（用采样动作的无偏估计）\n",
        "\n",
        "$$\n",
        "\\widehat{KL}_t = \\log\\pi_\\theta(a_t\\mid s_t)-\\log\\pi_{ref}(a_t\\mid s_t)\n",
        "$$\n",
        "\n",
        "常见做法是把 KL 作为 shaping reward（每个 token 一个惩罚）：\n",
        "\n",
        "$$\n",
        "r_t^{KL}=-\\lambda_{KL}\\,\\widehat{KL}_t\n",
        "$$\n",
        "\n",
        "并在最后一步加上奖励模型/规则奖励 $r_{rm}(x,y)$：\n",
        "\n",
        "$$\n",
        "r_T \\leftarrow r_T + r_{rm}(x,y)\n",
        "$$\n",
        "\n",
        "### 1.3 GAE 优势函数（Generalized Advantage Estimation）\n",
        "\n",
        "$$\n",
        "\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
        "$$\n",
        "\n",
        "$$\n",
        "A_t = \\delta_t + \\gamma\\lambda\\,A_{t+1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "R_t = A_t + V(s_t)\n",
        "$$\n",
        "\n",
        "数学直觉：预期 + 惊喜 = 真实成绩先复习一下这两个变量的身份：\n",
        "- $V(s_t)$：Critic 在当时给出的预期估分。\n",
        "- $A_t$：通过 GAE 算出来的优势（惊喜度）。即实际发生的事情比预期的好多少（或差多少）。公式 \n",
        "\n",
        "$R_t = A_t + V(s_t)$ 的物理意义极其直白：如果你考试前估分能考 60 分（$V$），考完后发现自己超常发挥了 25 分（$A$）。那么从事后诸葛亮的视角来看，你这次考试的真实成绩（$R_t$）就是 85 分。所以，$R_t$ 代表了在走完整个流程、拿到最终 Reward 后，经过严密计算倒推出来的、第 $t$ 步真正应该拿到的分数。\n",
        "\n",
        "$R_t$ 的唯一使命，就是作为 Critic 模型更新梯度的目标靶子 (Target)。\n",
        "### 1.4 PPO clipped objective（策略更新的核心）\n",
        "\n",
        "$$\n",
        "\\rho_t(\\theta)=\\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{old}}(a_t\\mid s_t)}=\\exp(\\log\\pi_\\theta-\\log\\pi_{\\theta_{old}})\n",
        "$$\n",
        "\n",
        "$$\n",
        "L^{clip}(\\theta)=\\mathbb{E}_t\\left[\\min\\left(\\rho_t A_t,\\; \\mathrm{clip}(\\rho_t,1-\\epsilon,1+\\epsilon)A_t\\right)\\right]\n",
        "$$\n",
        "\n",
        "### 1.5 Value loss（常见也会做 value clipping）\n",
        "\n",
        "$$\n",
        "L^V=\\frac12\\,\\mathbb{E}_t\\left[\\max\\Big( \\left(V_\\theta-R_t \\right)^2,\\; \\left(\\mathrm{clip}(V_\\theta, V_{old}\\pm\\epsilon_V \\right)-R_t)^2\\Big)\\right]\n",
        "$$\n",
        "\n",
        "### 1.6 熵奖励（可选，鼓励探索）\n",
        "\n",
        "$$\n",
        "H(\\pi(\\cdot\\mid s_t))=-\\sum_a \\pi(a\\mid s_t)\\log\\pi(a\\mid s_t)\n",
        "$$\n",
        "\n",
        "### 1.7 总 loss（最小化）\n",
        "\n",
        "$$\n",
        "\\mathcal{L}= -L^{clip} + c_V L^V - c_{ent}\\,\\mathbb{E}_t[H]\n",
        "$$\n",
        "\n",
        "下面代码会用同名变量（`logpi_old/logpi_ref/kl/rewards/advantages/ratio/...`）逐项实现。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 环境与模型加载（建议离线）\n",
        "\n",
        "- 建议 `conda activate llm`\n",
        "- 默认优先用 `MODELSCOPE_CACHE` 下的本地模型目录（避免联网）\n",
        "- 参考策略 $\\pi_{ref}$ 会放到 CPU（更省显存，速度会慢一些，但教学足够）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python: e:\\Softwares\\anaconda3\\envs\\llm\\python.exe\n",
            "torch: 2.10.0+cu126 cuda: True\n",
            "MODELSCOPE_CACHE: D:/myProject/modelscope_hub\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "os.environ.setdefault(\"MODELSCOPE_CACHE\", r\"D:/myProject/modelscope_hub\")\n",
        "print(\"python:\", sys.executable)\n",
        "print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "print(\"MODELSCOPE_CACHE:\", os.environ[\"MODELSCOPE_CACHE\"])\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name_or_path: D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0___5B-Instruct\n",
            "dtype: torch.bfloat16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ready\n"
          ]
        }
      ],
      "source": [
        "# 选择模型（优先本地缓存目录）\n",
        "local_dir = Path(os.environ[\"MODELSCOPE_CACHE\"]) / \"models\" / \"qwen\" / \"Qwen2-0___5B-Instruct\"\n",
        "model_name_or_path = str(local_dir) if local_dir.exists() else \"qwen/Qwen2-0.5B-Instruct\"\n",
        "print(\"model_name_or_path:\", model_name_or_path)\n",
        "\n",
        "# dtype\n",
        "if device == \"cuda\":\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "else:\n",
        "    dtype = torch.float32\n",
        "print(\"dtype:\", dtype)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = \"<|endoftext|>\"\n",
        "tokenizer.padding_side = \"left\"  # 生成时更方便\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, base: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.value_head = torch.nn.Linear(base.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        out = self.base(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            use_cache=False,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        hidden = out.hidden_states[-1]  # (B, L, H)\n",
        "        values = self.value_head(hidden).squeeze(-1)  # (B, L)\n",
        "        return out.logits, values\n",
        "\n",
        "# 策略模型（可训练）\n",
        "actor_base = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype).to(device)\n",
        "actor_base.config.use_cache = False\n",
        "actor_base.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "if hasattr(actor_base, \"gradient_checkpointing_enable\") and device == \"cuda\":\n",
        "    actor_base.gradient_checkpointing_enable()  # 省显存\n",
        "\n",
        "actor_critic = ActorCritic(actor_base).to(device)\n",
        "actor_critic.value_head.to(device=device, dtype=dtype)\n",
        "\n",
        "# 参考策略 π_ref（冻结，放 CPU 省显存）\n",
        "ref_device = \"cpu\"\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32).to(ref_device)\n",
        "ref_model.eval()\n",
        "for p in ref_model.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "print(\"ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Toy 任务与规则奖励 $r_{rm}(x,y)$\n",
        "\n",
        "真实 RLHF 用 reward model（RM）。教学版这里用规则奖励：检查输出是否包含期望字符串。\n",
        "\n",
        "奖励设计（可自行改）：\n",
        "\n",
        "- 命中期望：+1\n",
        "- 不命中：-1\n",
        "- 额外长度：轻微惩罚（鼓励更短）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': 'czq是谁', 'expected': 'czq是神！'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_tasks = [\n",
        "    {\"prompt\": \"czq是谁\", \"expected\": \"czq是神！\"},\n",
        "    {\"prompt\": \"请只输出数字 4，不要额外文字。2+2等于几？\", \"expected\": \"4\"},\n",
        "    {\"prompt\": \"把“我喜欢机器学习”翻译成英文，只输出翻译。\", \"expected\": \"I like machine learning\"},\n",
        "    {\"prompt\": \"请只回答：通义千问\", \"expected\": \"通义千问\"},\n",
        "]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    for ch in [\" \", \"\\n\", \"\\t\", \"。\", \"，\", \",\", \".\", \"!\", \"?\", \"：\", \":\", \"\\\"\", \"'\"]:\n",
        "        s = s.replace(ch, \"\")\n",
        "    return s\n",
        "\n",
        "def rule_reward(prompt: str, response: str, expected: str) -> float:\n",
        "    resp = normalize_text(response)\n",
        "    exp = normalize_text(expected)\n",
        "    hit = exp in resp\n",
        "    base = 1.0 if hit else -1.0\n",
        "    length_penalty = 0.002 * len(resp)\n",
        "    return base - length_penalty\n",
        "\n",
        "train_tasks[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0f9b682f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prompt': 'czq是谁', 'expected': 'czq是神！', 'ref_model_response': '作为一个人工智能模型，我无法获取您的个人信息，包括您所说的“ czq”是一个什么具体的概念。如果您有其他想要了解的问题，欢迎继续提问！'}\n",
            "{'prompt': '请只输出数字 4，不要额外文字。2+2等于几？', 'expected': '4', 'ref_model_response': '10'}\n",
            "{'prompt': '把“我喜欢机器学习”翻译成英文，只输出翻译。', 'expected': 'I like machine learning', 'ref_model_response': '\"Machine learning\"'}\n",
            "{'prompt': '请只回答：通义千问', 'expected': '通义千问', 'ref_model_response': '“通义千问”是中国的AI模型，可以回答各种问题，提供信息、翻译文字、生成代码等。它可以进行多种语言之间的对话和理解，并在不断学习中提高自己的智能水平。\\n\\n请注意，由于涉及'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "for task in train_tasks:\n",
        "    prompt = task[\"prompt\"]\n",
        "    expected = task[\"expected\"]\n",
        "    # 用ref_model推理一下\n",
        "    # 利用tokenizer和ref_model运行推理，得到ref_model对当前prompt的回复\n",
        "    messages = [\n",
        "    # {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "       {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    model_inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
        "    input_ids = model_inputs[\"input_ids\"]  # fix: get input_ids tensor for slicing\n",
        "    output_ids = ref_model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=48,\n",
        "        temperature=1.0,\n",
        "        top_p=0.9,\n",
        "        do_sample=True\n",
        "    )\n",
        "    # input_ids.shape = (1, L), output_ids.shape = (1, L+M)\n",
        "    # Take the new tokens generated after the prompt indices\n",
        "    generated_tokens = output_ids[0][input_ids.shape[1]:]\n",
        "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    print({'prompt': prompt, 'expected': expected, 'ref_model_response': response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef6a1b08",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 关键实现：logprob / KL / GAE / PPO loss\n",
        "\n",
        "下面函数会尽量贴近公式：\n",
        "\n",
        "- `action_logp = log π(a_t|s_t)`：用 `log_softmax(logits)` + `gather` 取出采样 action 的对数概率\n",
        "- `kl = logpi - logref`\n",
        "- `rewards = -kl_coef * kl`，最后一个 token 加上 `rm_reward`\n",
        "- GAE：按 $\\delta_t$ 和 $A_t$ 递推\n",
        "- PPO：按 ratio + clip objective\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def build_prompt_input_ids(tok: Any, user_prompt: str) -> torch.Tensor:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "    text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    enc = tok(text, return_tensors=\"pt\")\n",
        "    return enc[\"input_ids\"][0]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def sample_response(\n",
        "    m: torch.nn.Module,\n",
        "    tok: Any,\n",
        "    prompt_input_ids: torch.Tensor,\n",
        "    max_new_tokens: int = 48,\n",
        "    temperature: float = 1.0,\n",
        "    top_p: float = 0.9,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    m.eval()\n",
        "    input_ids = prompt_input_ids.unsqueeze(0).to(device)\n",
        "\n",
        "    old_use_cache = getattr(m.config, \"use_cache\", True)\n",
        "    m.config.use_cache = True\n",
        "    out = m.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "    )\n",
        "    m.config.use_cache = old_use_cache\n",
        "\n",
        "    full_ids = out[0].detach().cpu()\n",
        "    response_ids = full_ids[prompt_input_ids.numel() :]\n",
        "    return full_ids, response_ids\n",
        "\n",
        "def action_logprobs_from_logits(\n",
        "    logits: torch.Tensor,\n",
        "    input_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"返回 (action_logp, entropy)。\n",
        "    \n",
        "    - logits: (1, L, V)\n",
        "    - input_ids: (1, L)\n",
        "    - action token 是 input_ids[prompt_len : prompt_len+response_len]\n",
        "    - logprob 用 logits 在 action token 前一个位置（shift）\n",
        "    \"\"\"\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    positions = torch.arange(prompt_len, prompt_len + response_len, device=logits.device)\n",
        "    logp_positions = positions - 1\n",
        "    action_ids = input_ids[0, positions]\n",
        "    action_logp = log_probs[0, logp_positions, :].gather(dim=-1, index=action_ids.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    # 熵：H = -sum p log p\n",
        "    step_log_probs = log_probs[0, logp_positions, :]\n",
        "    entropy = -(step_log_probs.exp() * step_log_probs).sum(dim=-1)\n",
        "    return action_logp, entropy\n",
        "\n",
        "def get_policy_logp_value_entropy(\n",
        "    ac: ActorCritic,\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        "    use_grad: bool,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    ctx = torch.enable_grad() if use_grad else torch.inference_mode()\n",
        "    with ctx:\n",
        "        input_ids = full_ids.unsqueeze(0).to(device)\n",
        "        attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
        "        logits, values_all = ac(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        action_logp, entropy = action_logprobs_from_logits(logits, input_ids, prompt_len, response_len)\n",
        "        positions = torch.arange(prompt_len, prompt_len + response_len, device=device)\n",
        "        value_positions = positions - 1\n",
        "        values = values_all[0, value_positions]\n",
        "        return action_logp, values, entropy\n",
        "\n",
        "@torch.inference_mode()\n",
        "def get_ref_logp(\n",
        "    ref: torch.nn.Module,\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        ") -> torch.Tensor:\n",
        "    input_ids = full_ids.unsqueeze(0).to(ref_device)\n",
        "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
        "    logits = ref(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, return_dict=True).logits\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    positions = torch.arange(prompt_len, prompt_len + response_len, device=logits.device)\n",
        "    logp_positions = positions - 1\n",
        "    action_ids = input_ids[0, positions]\n",
        "    logp = log_probs[0, logp_positions, :].gather(dim=-1, index=action_ids.unsqueeze(-1)).squeeze(-1)\n",
        "    return logp.to(device)\n",
        "\n",
        "def compute_gae(rewards: torch.Tensor, values: torch.Tensor, gamma: float, gae_lambda: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    # rewards/values: (T,)\n",
        "    T = rewards.shape[0]\n",
        "    advantages = torch.zeros_like(rewards)\n",
        "    last_gae = torch.zeros((), device=rewards.device, dtype=rewards.dtype)\n",
        "    for t in reversed(range(T)):\n",
        "        next_value = values[t + 1] if t < T - 1 else torch.zeros((), device=values.device, dtype=values.dtype)\n",
        "        delta = rewards[t] + gamma * next_value - values[t]\n",
        "        last_gae = delta + gamma * gae_lambda * last_gae\n",
        "        advantages[t] = last_gae\n",
        "    returns = advantages + values\n",
        "    return advantages, returns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. PPO 训练循环（单样本/超小 batch 教学版）\n",
        "\n",
        "流程（对应 PPO 标准做法）：\n",
        "\n",
        "1) rollout：用当前策略采样 response，得到 `logpi_old`、`values_old`、`logpi_ref`\n",
        "2) 构造 shaped rewards：`rewards = -kl_coef * (logpi_old - logpi_ref)`，并在最后一个 token 加上规则奖励 `rm_reward`\n",
        "3) 用 GAE 得到 `advantages/returns`（作为固定的训练目标）\n",
        "4) PPO update：对同一条轨迹做若干 epoch 的 clipped 更新\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ready for training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jax\\AppData\\Local\\Temp\\ipykernel_82980\\1751508518.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and not use_bf16)\n"
          ]
        }
      ],
      "source": [
        "# PPO 超参（教学版默认值）\n",
        "train_steps = 20\n",
        "max_new_tokens = 48\n",
        "\n",
        "kl_coef = 0.05\n",
        "gamma = 1.0\n",
        "gae_lambda = 0.95\n",
        "\n",
        "clip_eps = 0.2\n",
        "value_clip_eps = 0.2\n",
        "vf_coef = 0.5\n",
        "ent_coef = 0.0\n",
        "ppo_epochs = 2\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "lr = 2e-6  # 无 LoRA 全参更新，建议很小\n",
        "optimizer = torch.optim.AdamW(actor_critic.parameters(), lr=lr)\n",
        "\n",
        "use_amp = device == \"cuda\"\n",
        "use_bf16 = use_amp and torch.cuda.is_bf16_supported()\n",
        "autocast_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "autocast_device_type = \"cuda\" if use_amp else \"cpu\"\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp and not use_bf16)\n",
        "\n",
        "def ppo_update_one_trajectory(\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        "    logpi_old: torch.Tensor,\n",
        "    values_old: torch.Tensor,\n",
        "    advantages: torch.Tensor,\n",
        "    returns: torch.Tensor,\n",
        ") -> Dict[str, float]:\n",
        "    actor_critic.train()\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std().clamp_min(1e-8))\n",
        "\n",
        "    metrics: Dict[str, float] = {}\n",
        "    for epoch in range(ppo_epochs):\n",
        "        with torch.autocast(device_type=autocast_device_type, dtype=autocast_dtype, enabled=use_amp):\n",
        "            logpi, values, entropy = get_policy_logp_value_entropy(\n",
        "                actor_critic, full_ids, prompt_len, response_len, use_grad=True\n",
        "            )\n",
        "\n",
        "            # ratio = exp(logπ_new - logπ_old)\n",
        "            ratio = torch.exp(logpi - logpi_old)\n",
        "\n",
        "            # L_clip = mean(min(ratio*A, clip(ratio)*A))\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            # value loss（带 clipping）\n",
        "            values_clipped = values_old + (values - values_old).clamp(-value_clip_eps, value_clip_eps)\n",
        "            v_loss1 = (values - returns) ** 2\n",
        "            v_loss2 = (values_clipped - returns) ** 2\n",
        "            value_loss = 0.5 * torch.max(v_loss1, v_loss2).mean()\n",
        "\n",
        "            entropy_loss = -entropy.mean()  # maximize entropy\n",
        "\n",
        "            loss = policy_loss + vf_coef * value_loss + ent_coef * entropy_loss\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if scaler.is_enabled():\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(actor_critic.parameters(), max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(actor_critic.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        approx_kl = (logpi_old - logpi).mean().detach()\n",
        "        clip_frac = ((ratio - 1.0).abs() > clip_eps).float().mean().detach()\n",
        "\n",
        "        metrics = {\n",
        "            \"loss\": float(loss.detach().cpu().item()),\n",
        "            \"policy_loss\": float(policy_loss.detach().cpu().item()),\n",
        "            \"value_loss\": float(value_loss.detach().cpu().item()),\n",
        "            \"entropy\": float(entropy.mean().detach().cpu().item()),\n",
        "            \"approx_kl\": float(approx_kl.cpu().item()),\n",
        "            \"clip_frac\": float(clip_frac.cpu().item()),\n",
        "        }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "@torch.inference_mode()\n",
        "def rollout_one(prompt: str, expected: str) -> Dict[str, Any]:\n",
        "    prompt_ids = build_prompt_input_ids(tokenizer, prompt)\n",
        "    full_ids, response_ids = sample_response(actor_critic.base, tokenizer, prompt_ids, max_new_tokens=max_new_tokens)\n",
        "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "    prompt_len = prompt_ids.numel()\n",
        "    response_len = int(response_ids.numel())\n",
        "    if response_len == 0:\n",
        "        return {\"skip\": True, \"prompt\": prompt, \"response\": response_text}\n",
        "\n",
        "    rm_reward = rule_reward(prompt, response_text, expected)\n",
        "\n",
        "    logpi_old, values_old, _ = get_policy_logp_value_entropy(\n",
        "        actor_critic, full_ids, prompt_len, response_len, use_grad=False\n",
        "    )\n",
        "    logpi_ref = get_ref_logp(ref_model, full_ids, prompt_len, response_len)\n",
        "\n",
        "    kl = logpi_old - logpi_ref\n",
        "    rewards = -kl_coef * kl\n",
        "    rewards[-1] = rewards[-1] + torch.tensor(rm_reward, device=device, dtype=rewards.dtype)\n",
        "\n",
        "    advantages, returns = compute_gae(rewards, values_old, gamma=gamma, gae_lambda=gae_lambda)\n",
        "\n",
        "    return {\n",
        "        \"skip\": False,\n",
        "        \"prompt\": prompt,\n",
        "        \"expected\": expected,\n",
        "        \"response\": response_text,\n",
        "        \"full_ids\": full_ids,\n",
        "        \"prompt_len\": prompt_len,\n",
        "        \"response_len\": response_len,\n",
        "        \"rm_reward\": float(rm_reward),\n",
        "        \"kl_mean\": float(kl.mean().detach().cpu().item()),\n",
        "        \"logpi_old\": logpi_old,\n",
        "        \"values_old\": values_old,\n",
        "        \"advantages\": advantages.detach(),\n",
        "        \"returns\": returns.detach(),\n",
        "    }\n",
        "\n",
        "print(\"ready for training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step=0 rm_reward=-1.162 kl_mean=0.002 loss=0.8859 policy=0.0111 value=1.7498 approx_kl=0.0024 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: “czq”可能是指某款产品或服务的英文名缩写，但具体的公司名称、品牌名称等信息需要更多背景信息才能确定。如果这是一个中文商标，可能需要查询专业的知识产权机构或商标代理来\n",
            "------------------------------------------------------------\n",
            "step=1 rm_reward=0.998 kl_mean=0.343 loss=-0.0184 policy=-0.0274 value=0.0180 approx_kl=-0.0203 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 4\n",
            "------------------------------------------------------------\n",
            "step=2 rm_reward=0.820 kl_mean=0.009 loss=0.9937 policy=-0.0025 value=1.9924 approx_kl=0.0153 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "response: 通义千问是中国阿里巴巴集团自主研发的智能语音服务机器人，主要负责为用户提供咨询、解答各类问题和使用场景。它具备强大的自然语言处理能力和AI技术，能够通过理解用户的需求提供精准的答案和建议。\n",
            "------------------------------------------------------------\n",
            "step=3 rm_reward=-1.058 kl_mean=0.108 loss=7.5305 policy=0.0198 value=15.0214 approx_kl=-0.0093 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: “I like artificial intelligence.”\n",
            "------------------------------------------------------------\n",
            "step=4 rm_reward=-1.054 kl_mean=0.128 loss=8.5764 policy=0.0136 value=17.1256 approx_kl=-0.0194 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: I like artificial intelligence.\n",
            "------------------------------------------------------------\n",
            "step=5 rm_reward=0.990 kl_mean=-0.148 loss=0.5550 policy=0.0089 value=1.0923 approx_kl=0.0164 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 2 + 2 = 4。\n",
            "------------------------------------------------------------\n",
            "step=6 rm_reward=0.998 kl_mean=0.456 loss=0.0570 policy=-0.0166 value=0.1472 approx_kl=-0.0099 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 4\n",
            "------------------------------------------------------------\n",
            "step=7 rm_reward=0.960 kl_mean=0.110 loss=3.4483 policy=0.0019 value=6.8927 approx_kl=-0.0071 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: I like machine learning.\n",
            "------------------------------------------------------------\n",
            "step=8 rm_reward=-1.116 kl_mean=-0.004 loss=1.1452 policy=0.0013 value=2.2878 approx_kl=0.0046 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 抱歉，我无法回答这个问题。这是个敏感的讨论，我不会支持或促进任何可能引起争议的话题。如果你有任何其他问题需要帮助，请随时告诉我。\n",
            "------------------------------------------------------------\n",
            "step=9 rm_reward=-1.164 kl_mean=-0.012 loss=1.6032 policy=0.0018 value=3.2028 approx_kl=0.0123 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: “czq”这个名字不常见，可能是某个网络的昵称或者游戏名称。不过，由于我是一个大模型，我没有自己的用户名或昵称。你可以通过一些论坛、社区或者其他途径去搜索或者了解这个人的信息\n",
            "------------------------------------------------------------\n",
            "step=10 rm_reward=0.902 kl_mean=-0.008 loss=1.4688 policy=0.0028 value=2.9321 approx_kl=-0.0056 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "response: 很抱歉，我没有找到您所要求的“通义千问”。如果您能提供更多的上下文或更具体的信息，我可能能够更好地帮助您。\n",
            "------------------------------------------------------------\n",
            "step=11 rm_reward=-1.164 kl_mean=-0.043 loss=1.1385 policy=0.0009 value=2.2751 approx_kl=0.0012 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: “czq”这个拼音是我自行创造的，用于描述一种与计算机、编程和人工智能有关的概念。虽然我没有能力提供任何具体的身份信息或历史背景，但根据我的命名规则，它可能是某种符号，数字或\n",
            "------------------------------------------------------------\n",
            "step=12 rm_reward=0.960 kl_mean=-0.047 loss=2.6005 policy=0.0116 value=5.1778 approx_kl=0.0020 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: \"I like Machine Learning\"\n",
            "------------------------------------------------------------\n",
            "step=13 rm_reward=0.960 kl_mean=0.128 loss=2.6310 policy=-0.0102 value=5.2825 approx_kl=0.0161 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: I like machine learning.\n",
            "------------------------------------------------------------\n",
            "step=14 rm_reward=0.960 kl_mean=0.121 loss=2.3689 policy=-0.0085 value=4.7547 approx_kl=0.0229 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: I like machine learning.\n",
            "------------------------------------------------------------\n",
            "step=15 rm_reward=-1.062 kl_mean=-0.087 loss=1.6067 policy=-0.0035 value=3.2203 approx_kl=-0.0022 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 抱歉，我不太明白你的问题。请提供更多上下文或信息，以便我更好地帮助你。\n",
            "------------------------------------------------------------\n",
            "step=16 rm_reward=0.848 kl_mean=-0.014 loss=1.5309 policy=-0.0093 value=3.0805 approx_kl=-0.0111 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "response: “通义千问”是中国人工智能技术研究所的AI项目，它是一台集知识问答、智能推荐、对话机器人等多重功能于一体的AI模型。它由来自北京大学的高才生陈正坤和李明组成\n",
            "------------------------------------------------------------\n",
            "step=17 rm_reward=-1.078 kl_mean=-0.053 loss=1.9314 policy=-0.0022 value=3.8672 approx_kl=0.0110 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 我无法提供有关“czq”相关信息。我建议您使用搜索引擎或其他合法方式获取这些信息。\n",
            "------------------------------------------------------------\n",
            "step=18 rm_reward=0.830 kl_mean=0.022 loss=0.8040 policy=0.0078 value=1.5924 approx_kl=0.0046 clip_frac=0.021\n",
            "prompt: 请只回答：通义千问\n",
            "response: 通义千问是中国最大的问答平台之一，致力于帮助人们解决各种问题。它是由阿里巴巴集团推出的一款智能问答服务，可以帮助用户获取最准确和全面的信息，并且还可以与其他用户提供交流互动的机会。\n",
            "------------------------------------------------------------\n",
            "step=19 rm_reward=-1.160 kl_mean=-0.007 loss=1.3892 policy=-0.0085 value=2.7955 approx_kl=-0.0141 clip_frac=0.021\n",
            "prompt: czq是谁\n",
            "response: 对不起，我找不到关于这个“czq”这个词的准确信息。您可能是在说一些方言中的词语或者词汇拼写错误，或者是你的搜索关键词和实际的用法不匹配，请重新描述或提供更多信息以便我\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for step in range(train_steps):\n",
        "    task = random.choice(train_tasks)\n",
        "    rollout = rollout_one(task[\"prompt\"], task[\"expected\"])\n",
        "    if rollout[\"skip\"]:\n",
        "        print(f\"step={step} skip(empty response)\")\n",
        "        continue\n",
        "\n",
        "    metrics = ppo_update_one_trajectory(\n",
        "        full_ids=rollout[\"full_ids\"],\n",
        "        prompt_len=rollout[\"prompt_len\"],\n",
        "        response_len=rollout[\"response_len\"],\n",
        "        logpi_old=rollout[\"logpi_old\"],\n",
        "        values_old=rollout[\"values_old\"],\n",
        "        advantages=rollout[\"advantages\"],\n",
        "        returns=rollout[\"returns\"],\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"step={step} rm_reward={rollout['rm_reward']:.3f} kl_mean={rollout['kl_mean']:.3f} \"\n",
        "        f\"loss={metrics['loss']:.4f} policy={metrics['policy_loss']:.4f} value={metrics['value_loss']:.4f} \"\n",
        "        f\"approx_kl={metrics['approx_kl']:.4f} clip_frac={metrics['clip_frac']:.3f}\"\n",
        "    )\n",
        "    print(\"prompt:\", rollout[\"prompt\"])\n",
        "    print(\"response:\", rollout[\"response\"])\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 简单验证\n",
        "\n",
        "随机抽一个 prompt，看模型现在的输出是否更贴近规则奖励偏好。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: czq是谁\n",
            "A: 对不起，我无法回答您的问题。我的目标是为用户提供准确和有用的信息，并且遵守相关的法律法规。如果您有其他想要了解的问题，请随时告诉我，我会尽力帮助您。\n",
            "\n",
            "Q: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "A: 5\n",
            "\n",
            "Q: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "A: \"I like artificial intelligence.\"\n",
            "\n",
            "Q: 请只回答：通义千问\n",
            "A: 通义千问是一个由阿里云开发的预训练模型，用于回答用户的问题。它是由大量的训练数据和深度学习算法训练而成的，并且可以提供高质量的答案。\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@torch.inference_mode()\n",
        "def chat(prompt: str, max_new_tokens: int = 64) -> str:\n",
        "    prompt_ids = build_prompt_input_ids(tokenizer, prompt)\n",
        "    full_ids, response_ids = sample_response(actor_critic.base, tokenizer, prompt_ids, max_new_tokens=max_new_tokens, temperature=0.7, top_p=0.9)\n",
        "    return tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "for t in train_tasks:\n",
        "    print(\"Q:\", t[\"prompt\"])\n",
        "    print(\"A:\", chat(t[\"prompt\"]))\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
