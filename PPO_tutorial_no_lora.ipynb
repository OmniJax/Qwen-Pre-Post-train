{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO 教学（手写公式 + 最小实现，无 LoRA）\n",
        "\n",
        "本 Notebook 目标：用尽量少的现成“PPO/RLHF Trainer”库（不使用 TRL），用 PyTorch 手写 PPO 的关键计算，并让代码变量名能对应公式。\n",
        "\n",
        "注意：\n",
        "\n",
        "- 这是教学最小实现：只做单卡、超小 batch、toy reward（规则奖励），便于看懂每一步。\n",
        "- “无 LoRA”表示会更新基座模型全部参数，显存要求最高；如果显存不够，请用后面的 LoRA 版。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. PPO（token 级）核心公式\n",
        "\n",
        "把语言模型生成看成一个序列决策过程：\n",
        "\n",
        "- prompt 为 $x$，模型生成 response $y=(a_1,\\dots,a_T)$\n",
        "- 每个 token $a_t$ 是一步 action；状态 $s_t=(x,a_{<t})$\n",
        "- 训练策略（actor）为 $\\pi_\\theta$；rollout 时的旧策略为 $\\pi_{\\theta_{old}}$\n",
        "- 参考策略（冻结，用于 KL 约束）为 $\\pi_{ref}$\n",
        "- 价值函数（critic）为 $V_\\theta(s_t)$（这里用一个 value head 预测）\n",
        "\n",
        "### 1.1 token 对数概率（实现里会显式算 log_softmax + gather）\n",
        "\n",
        "$$\n",
        "\\log \\pi_\\theta(y\\mid x)=\\sum_{t=1}^{T} \\log \\pi_\\theta(a_t\\mid s_t)\n",
        "$$\n",
        "\n",
        "### 1.2 KL 约束（用采样动作的无偏估计）\n",
        "\n",
        "$$\n",
        "\\widehat{KL}_t = \\log\\pi_\\theta(a_t\\mid s_t)-\\log\\pi_{ref}(a_t\\mid s_t)\n",
        "$$\n",
        "\n",
        "常见做法是把 KL 作为 shaping reward（每个 token 一个惩罚）：\n",
        "\n",
        "$$\n",
        "r_t^{KL}=-\\lambda_{KL}\\,\\widehat{KL}_t\n",
        "$$\n",
        "\n",
        "并在最后一步加上奖励模型/规则奖励 $r_{rm}(x,y)$：\n",
        "\n",
        "$$\n",
        "r_T \\leftarrow r_T + r_{rm}(x,y)\n",
        "$$\n",
        "\n",
        "### 1.3 GAE 优势函数（Generalized Advantage Estimation）\n",
        "\n",
        "$$\n",
        "\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
        "$$\n",
        "\n",
        "$$\n",
        "A_t = \\delta_t + \\gamma\\lambda\\,A_{t+1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "R_t = A_t + V(s_t)\n",
        "$$\n",
        "\n",
        "### 1.4 PPO clipped objective（策略更新的核心）\n",
        "\n",
        "$$\n",
        "\\rho_t(\\theta)=\\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{old}}(a_t\\mid s_t)}=\\exp(\\log\\pi_\\theta-\\log\\pi_{\\theta_{old}})\n",
        "$$\n",
        "\n",
        "$$\n",
        "L^{clip}(\\theta)=\\mathbb{E}_t\\left[\\min\\left(\\rho_t A_t,\\; \\mathrm{clip}(\\rho_t,1-\\epsilon,1+\\epsilon)A_t\\right)\\right]\n",
        "$$\n",
        "\n",
        "### 1.5 Value loss（常见也会做 value clipping）\n",
        "\n",
        "$$\n",
        "L^V=\\frac12\\,\\mathbb{E}_t\\left[\\max\\Big((V_\\theta-R_t)^2,\\; (\\mathrm{clip}(V_\\theta, V_{old}\\pm\\epsilon_V)-R_t)^2\\Big)\\right]\n",
        "$$\n",
        "\n",
        "### 1.6 熵奖励（可选，鼓励探索）\n",
        "\n",
        "$$\n",
        "H(\\pi(\\cdot\\mid s_t))=-\\sum_a \\pi(a\\mid s_t)\\log\\pi(a\\mid s_t)\n",
        "$$\n",
        "\n",
        "### 1.7 总 loss（最小化）\n",
        "\n",
        "$$\n",
        "\\mathcal{L}= -L^{clip} + c_V L^V - c_{ent}\\,\\mathbb{E}_t[H]\n",
        "$$\n",
        "\n",
        "下面代码会用同名变量（`logpi_old/logpi_ref/kl/rewards/advantages/ratio/...`）逐项实现。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 环境与模型加载（建议离线）\n",
        "\n",
        "- 建议 `conda activate llm`\n",
        "- 默认优先用 `MODELSCOPE_CACHE` 下的本地模型目录（避免联网）\n",
        "- 参考策略 $\\pi_{ref}$ 会放到 CPU（更省显存，速度会慢一些，但教学足够）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "os.environ.setdefault(\"MODELSCOPE_CACHE\", r\"D:/myProject/modelscope_hub\")\n",
        "print(\"python:\", sys.executable)\n",
        "print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "print(\"MODELSCOPE_CACHE:\", os.environ[\"MODELSCOPE_CACHE\"])\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 选择模型（优先本地缓存目录）\n",
        "local_dir = Path(os.environ[\"MODELSCOPE_CACHE\"]) / \"models\" / \"qwen\" / \"Qwen2-0___5B-Instruct\"\n",
        "model_name_or_path = str(local_dir) if local_dir.exists() else \"qwen/Qwen2-0.5B-Instruct\"\n",
        "print(\"model_name_or_path:\", model_name_or_path)\n",
        "\n",
        "# dtype\n",
        "if device == \"cuda\":\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "else:\n",
        "    dtype = torch.float32\n",
        "print(\"dtype:\", dtype)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = \"<|endoftext|>\"\n",
        "tokenizer.padding_side = \"left\"  # 生成时更方便\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, base: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.value_head = torch.nn.Linear(base.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        out = self.base(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            use_cache=False,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        hidden = out.hidden_states[-1]  # (B, L, H)\n",
        "        values = self.value_head(hidden).squeeze(-1)  # (B, L)\n",
        "        return out.logits, values\n",
        "\n",
        "# 策略模型（可训练）\n",
        "actor_base = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype).to(device)\n",
        "actor_base.config.use_cache = False\n",
        "actor_base.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "if hasattr(actor_base, \"gradient_checkpointing_enable\") and device == \"cuda\":\n",
        "    actor_base.gradient_checkpointing_enable()  # 省显存\n",
        "\n",
        "actor_critic = ActorCritic(actor_base).to(device)\n",
        "actor_critic.value_head.to(device=device, dtype=dtype)\n",
        "\n",
        "# 参考策略 π_ref（冻结，放 CPU 省显存）\n",
        "ref_device = \"cpu\"\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32).to(ref_device)\n",
        "ref_model.eval()\n",
        "for p in ref_model.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "print(\"ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Toy 任务与规则奖励 $r_{rm}(x,y)$\n",
        "\n",
        "真实 RLHF 用 reward model（RM）。教学版这里用规则奖励：检查输出是否包含期望字符串。\n",
        "\n",
        "奖励设计（可自行改）：\n",
        "\n",
        "- 命中期望：+1\n",
        "- 不命中：-1\n",
        "- 额外长度：轻微惩罚（鼓励更短）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_tasks = [\n",
        "    {\"prompt\": \"请只回答一个词：小鱼儿\", \"expected\": \"小鱼儿\"},\n",
        "    {\"prompt\": \"请只输出数字 4，不要额外文字。2+2等于几？\", \"expected\": \"4\"},\n",
        "    {\"prompt\": \"把“我喜欢机器学习”翻译成英文，只输出翻译。\", \"expected\": \"I like machine learning\"},\n",
        "    {\"prompt\": \"请只回答：通义千问\", \"expected\": \"通义千问\"},\n",
        "]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    for ch in [\" \", \"\\n\", \"\\t\", \"。\", \"，\", \",\", \".\", \"!\", \"?\", \"：\", \":\", \"\\\"\", \"'\"]:\n",
        "        s = s.replace(ch, \"\")\n",
        "    return s\n",
        "\n",
        "def rule_reward(prompt: str, response: str, expected: str) -> float:\n",
        "    resp = normalize_text(response)\n",
        "    exp = normalize_text(expected)\n",
        "    hit = exp in resp\n",
        "    base = 1.0 if hit else -1.0\n",
        "    length_penalty = 0.002 * len(resp)\n",
        "    return base - length_penalty\n",
        "\n",
        "train_tasks[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 关键实现：logprob / KL / GAE / PPO loss\n",
        "\n",
        "下面函数会尽量贴近公式：\n",
        "\n",
        "- `action_logp = log π(a_t|s_t)`：用 `log_softmax(logits)` + `gather` 取出采样 action 的对数概率\n",
        "- `kl = logpi - logref`\n",
        "- `rewards = -kl_coef * kl`，最后一个 token 加上 `rm_reward`\n",
        "- GAE：按 $\\delta_t$ 和 $A_t$ 递推\n",
        "- PPO：按 ratio + clip objective\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def build_prompt_input_ids(tok: Any, user_prompt: str) -> torch.Tensor:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "    text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    enc = tok(text, return_tensors=\"pt\")\n",
        "    return enc[\"input_ids\"][0]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def sample_response(\n",
        "    m: torch.nn.Module,\n",
        "    tok: Any,\n",
        "    prompt_input_ids: torch.Tensor,\n",
        "    max_new_tokens: int = 48,\n",
        "    temperature: float = 1.0,\n",
        "    top_p: float = 0.9,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    m.eval()\n",
        "    input_ids = prompt_input_ids.unsqueeze(0).to(device)\n",
        "\n",
        "    old_use_cache = getattr(m.config, \"use_cache\", True)\n",
        "    m.config.use_cache = True\n",
        "    out = m.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "    )\n",
        "    m.config.use_cache = old_use_cache\n",
        "\n",
        "    full_ids = out[0].detach().cpu()\n",
        "    response_ids = full_ids[prompt_input_ids.numel() :]\n",
        "    return full_ids, response_ids\n",
        "\n",
        "def action_logprobs_from_logits(\n",
        "    logits: torch.Tensor,\n",
        "    input_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"返回 (action_logp, entropy)。\n",
        "    \n",
        "    - logits: (1, L, V)\n",
        "    - input_ids: (1, L)\n",
        "    - action token 是 input_ids[prompt_len : prompt_len+response_len]\n",
        "    - logprob 用 logits 在 action token 前一个位置（shift）\n",
        "    \"\"\"\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    positions = torch.arange(prompt_len, prompt_len + response_len, device=logits.device)\n",
        "    logp_positions = positions - 1\n",
        "    action_ids = input_ids[0, positions]\n",
        "    action_logp = log_probs[0, logp_positions, :].gather(dim=-1, index=action_ids.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    # 熵：H = -sum p log p\n",
        "    step_log_probs = log_probs[0, logp_positions, :]\n",
        "    entropy = -(step_log_probs.exp() * step_log_probs).sum(dim=-1)\n",
        "    return action_logp, entropy\n",
        "\n",
        "def get_policy_logp_value_entropy(\n",
        "    ac: ActorCritic,\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        "    use_grad: bool,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    ctx = torch.enable_grad() if use_grad else torch.inference_mode()\n",
        "    with ctx:\n",
        "        input_ids = full_ids.unsqueeze(0).to(device)\n",
        "        attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
        "        logits, values_all = ac(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        action_logp, entropy = action_logprobs_from_logits(logits, input_ids, prompt_len, response_len)\n",
        "        positions = torch.arange(prompt_len, prompt_len + response_len, device=device)\n",
        "        value_positions = positions - 1\n",
        "        values = values_all[0, value_positions]\n",
        "        return action_logp, values, entropy\n",
        "\n",
        "@torch.inference_mode()\n",
        "def get_ref_logp(\n",
        "    ref: torch.nn.Module,\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        ") -> torch.Tensor:\n",
        "    input_ids = full_ids.unsqueeze(0).to(ref_device)\n",
        "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
        "    logits = ref(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, return_dict=True).logits\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    positions = torch.arange(prompt_len, prompt_len + response_len, device=logits.device)\n",
        "    logp_positions = positions - 1\n",
        "    action_ids = input_ids[0, positions]\n",
        "    logp = log_probs[0, logp_positions, :].gather(dim=-1, index=action_ids.unsqueeze(-1)).squeeze(-1)\n",
        "    return logp.to(device)\n",
        "\n",
        "def compute_gae(rewards: torch.Tensor, values: torch.Tensor, gamma: float, gae_lambda: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    # rewards/values: (T,)\n",
        "    T = rewards.shape[0]\n",
        "    advantages = torch.zeros_like(rewards)\n",
        "    last_gae = torch.zeros((), device=rewards.device, dtype=rewards.dtype)\n",
        "    for t in reversed(range(T)):\n",
        "        next_value = values[t + 1] if t < T - 1 else torch.zeros((), device=values.device, dtype=values.dtype)\n",
        "        delta = rewards[t] + gamma * next_value - values[t]\n",
        "        last_gae = delta + gamma * gae_lambda * last_gae\n",
        "        advantages[t] = last_gae\n",
        "    returns = advantages + values\n",
        "    return advantages, returns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. PPO 训练循环（单样本/超小 batch 教学版）\n",
        "\n",
        "流程（对应 PPO 标准做法）：\n",
        "\n",
        "1) rollout：用当前策略采样 response，得到 `logpi_old`、`values_old`、`logpi_ref`\n",
        "2) 构造 shaped rewards：`rewards = -kl_coef * (logpi_old - logpi_ref)`，并在最后一个 token 加上规则奖励 `rm_reward`\n",
        "3) 用 GAE 得到 `advantages/returns`（作为固定的训练目标）\n",
        "4) PPO update：对同一条轨迹做若干 epoch 的 clipped 更新\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PPO 超参（教学版默认值）\n",
        "train_steps = 20\n",
        "max_new_tokens = 48\n",
        "\n",
        "kl_coef = 0.05\n",
        "gamma = 1.0\n",
        "gae_lambda = 0.95\n",
        "\n",
        "clip_eps = 0.2\n",
        "value_clip_eps = 0.2\n",
        "vf_coef = 0.5\n",
        "ent_coef = 0.0\n",
        "ppo_epochs = 2\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "lr = 2e-6  # 无 LoRA 全参更新，建议很小\n",
        "optimizer = torch.optim.AdamW(actor_critic.parameters(), lr=lr)\n",
        "\n",
        "use_amp = device == \"cuda\"\n",
        "use_bf16 = use_amp and torch.cuda.is_bf16_supported()\n",
        "autocast_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "autocast_device_type = \"cuda\" if use_amp else \"cpu\"\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp and not use_bf16)\n",
        "\n",
        "def ppo_update_one_trajectory(\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        "    logpi_old: torch.Tensor,\n",
        "    values_old: torch.Tensor,\n",
        "    advantages: torch.Tensor,\n",
        "    returns: torch.Tensor,\n",
        ") -> Dict[str, float]:\n",
        "    actor_critic.train()\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std().clamp_min(1e-8))\n",
        "\n",
        "    metrics: Dict[str, float] = {}\n",
        "    for epoch in range(ppo_epochs):\n",
        "        with torch.autocast(device_type=autocast_device_type, dtype=autocast_dtype, enabled=use_amp):\n",
        "            logpi, values, entropy = get_policy_logp_value_entropy(\n",
        "                actor_critic, full_ids, prompt_len, response_len, use_grad=True\n",
        "            )\n",
        "\n",
        "            # ratio = exp(logπ_new - logπ_old)\n",
        "            ratio = torch.exp(logpi - logpi_old)\n",
        "\n",
        "            # L_clip = mean(min(ratio*A, clip(ratio)*A))\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            # value loss（带 clipping）\n",
        "            values_clipped = values_old + (values - values_old).clamp(-value_clip_eps, value_clip_eps)\n",
        "            v_loss1 = (values - returns) ** 2\n",
        "            v_loss2 = (values_clipped - returns) ** 2\n",
        "            value_loss = 0.5 * torch.max(v_loss1, v_loss2).mean()\n",
        "\n",
        "            entropy_loss = -entropy.mean()  # maximize entropy\n",
        "\n",
        "            loss = policy_loss + vf_coef * value_loss + ent_coef * entropy_loss\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if scaler.is_enabled():\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(actor_critic.parameters(), max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(actor_critic.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        approx_kl = (logpi_old - logpi).mean().detach()\n",
        "        clip_frac = ((ratio - 1.0).abs() > clip_eps).float().mean().detach()\n",
        "\n",
        "        metrics = {\n",
        "            \"loss\": float(loss.detach().cpu().item()),\n",
        "            \"policy_loss\": float(policy_loss.detach().cpu().item()),\n",
        "            \"value_loss\": float(value_loss.detach().cpu().item()),\n",
        "            \"entropy\": float(entropy.mean().detach().cpu().item()),\n",
        "            \"approx_kl\": float(approx_kl.cpu().item()),\n",
        "            \"clip_frac\": float(clip_frac.cpu().item()),\n",
        "        }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "@torch.inference_mode()\n",
        "def rollout_one(prompt: str, expected: str) -> Dict[str, Any]:\n",
        "    prompt_ids = build_prompt_input_ids(tokenizer, prompt)\n",
        "    full_ids, response_ids = sample_response(actor_critic.base, tokenizer, prompt_ids, max_new_tokens=max_new_tokens)\n",
        "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "    prompt_len = prompt_ids.numel()\n",
        "    response_len = int(response_ids.numel())\n",
        "    if response_len == 0:\n",
        "        return {\"skip\": True, \"prompt\": prompt, \"response\": response_text}\n",
        "\n",
        "    rm_reward = rule_reward(prompt, response_text, expected)\n",
        "\n",
        "    logpi_old, values_old, _ = get_policy_logp_value_entropy(\n",
        "        actor_critic, full_ids, prompt_len, response_len, use_grad=False\n",
        "    )\n",
        "    logpi_ref = get_ref_logp(ref_model, full_ids, prompt_len, response_len)\n",
        "\n",
        "    kl = logpi_old - logpi_ref\n",
        "    rewards = -kl_coef * kl\n",
        "    rewards[-1] = rewards[-1] + torch.tensor(rm_reward, device=device, dtype=rewards.dtype)\n",
        "\n",
        "    advantages, returns = compute_gae(rewards, values_old, gamma=gamma, gae_lambda=gae_lambda)\n",
        "\n",
        "    return {\n",
        "        \"skip\": False,\n",
        "        \"prompt\": prompt,\n",
        "        \"expected\": expected,\n",
        "        \"response\": response_text,\n",
        "        \"full_ids\": full_ids,\n",
        "        \"prompt_len\": prompt_len,\n",
        "        \"response_len\": response_len,\n",
        "        \"rm_reward\": float(rm_reward),\n",
        "        \"kl_mean\": float(kl.mean().detach().cpu().item()),\n",
        "        \"logpi_old\": logpi_old,\n",
        "        \"values_old\": values_old,\n",
        "        \"advantages\": advantages.detach(),\n",
        "        \"returns\": returns.detach(),\n",
        "    }\n",
        "\n",
        "print(\"ready for training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for step in range(train_steps):\n",
        "    task = random.choice(train_tasks)\n",
        "    rollout = rollout_one(task[\"prompt\"], task[\"expected\"])\n",
        "    if rollout[\"skip\"]:\n",
        "        print(f\"step={step} skip(empty response)\")\n",
        "        continue\n",
        "\n",
        "    metrics = ppo_update_one_trajectory(\n",
        "        full_ids=rollout[\"full_ids\"],\n",
        "        prompt_len=rollout[\"prompt_len\"],\n",
        "        response_len=rollout[\"response_len\"],\n",
        "        logpi_old=rollout[\"logpi_old\"],\n",
        "        values_old=rollout[\"values_old\"],\n",
        "        advantages=rollout[\"advantages\"],\n",
        "        returns=rollout[\"returns\"],\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"step={step} rm_reward={rollout['rm_reward']:.3f} kl_mean={rollout['kl_mean']:.3f} \"\n",
        "        f\"loss={metrics['loss']:.4f} policy={metrics['policy_loss']:.4f} value={metrics['value_loss']:.4f} \"\n",
        "        f\"approx_kl={metrics['approx_kl']:.4f} clip_frac={metrics['clip_frac']:.3f}\"\n",
        "    )\n",
        "    print(\"prompt:\", rollout[\"prompt\"])\n",
        "    print(\"response:\", rollout[\"response\"])\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 简单验证\n",
        "\n",
        "随机抽一个 prompt，看模型现在的输出是否更贴近规则奖励偏好。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def chat(prompt: str, max_new_tokens: int = 64) -> str:\n",
        "    prompt_ids = build_prompt_input_ids(tokenizer, prompt)\n",
        "    full_ids, response_ids = sample_response(actor_critic.base, tokenizer, prompt_ids, max_new_tokens=max_new_tokens, temperature=0.7, top_p=0.9)\n",
        "    return tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "for t in train_tasks:\n",
        "    print(\"Q:\", t[\"prompt\"])\n",
        "    print(\"A:\", chat(t[\"prompt\"]))\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
