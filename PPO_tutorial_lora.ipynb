{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO 教学（手写公式 + 最小实现，手写 LoRA）\n",
        "\n",
        "这一版与 `PPO_tutorial_no_lora.ipynb` 的区别：\n",
        "\n",
        "- 冻结基座模型参数\n",
        "- 在若干 Linear 层上手写注入 LoRA（不使用 peft/trl）\n",
        "- 只训练 LoRA 参数 + value head\n",
        "\n",
        "优点：显存更省、训练更容易跑通；缺点：表达能力受限于低秩增量。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. PPO（token 级）核心公式（与无 LoRA 版相同）\n",
        "\n",
        "符号与公式同 `PPO_tutorial_no_lora.ipynb`，这里不再重复推导，只强调：\n",
        "\n",
        "- 我们仍然在 token 级计算 `logπ_old/logπ_ref/KL/GAE/ratio/clip objective`\n",
        "- LoRA 只改变“哪些参数可训练”，不改变 PPO 的数学形式\n",
        "\n",
        "总 loss：\n",
        "\n",
        "$$\n",
        "\\mathcal{L}= -L^{clip} + c_V L^V - c_{ent}\\,\\mathbb{E}_t[H]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 环境与模型加载（建议离线）\n",
        "\n",
        "- 建议 `conda activate llm`\n",
        "- 默认优先用 `MODELSCOPE_CACHE` 下的本地模型目录（避免联网）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "os.environ.setdefault(\"MODELSCOPE_CACHE\", r\"D:/myProject/modelscope_hub\")\n",
        "print(\"python:\", sys.executable)\n",
        "print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "print(\"MODELSCOPE_CACHE:\", os.environ[\"MODELSCOPE_CACHE\"])\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 选择模型（优先本地缓存目录）\n",
        "local_dir = Path(os.environ[\"MODELSCOPE_CACHE\"]) / \"models\" / \"qwen\" / \"Qwen2-0___5B-Instruct\"\n",
        "model_name_or_path = str(local_dir) if local_dir.exists() else \"qwen/Qwen2-0.5B-Instruct\"\n",
        "print(\"model_name_or_path:\", model_name_or_path)\n",
        "\n",
        "if device == \"cuda\":\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "else:\n",
        "    dtype = torch.float32\n",
        "print(\"dtype:\", dtype)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = \"<|endoftext|>\"\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, base: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.value_head = torch.nn.Linear(base.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        out = self.base(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            use_cache=False,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        hidden = out.hidden_states[-1]\n",
        "        values = self.value_head(hidden).squeeze(-1)\n",
        "        return out.logits, values\n",
        "\n",
        "actor_base = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype).to(device)\n",
        "actor_base.config.use_cache = False\n",
        "actor_base.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "if hasattr(actor_base, \"gradient_checkpointing_enable\") and device == \"cuda\":\n",
        "    actor_base.gradient_checkpointing_enable()\n",
        "\n",
        "actor_critic = ActorCritic(actor_base).to(device)\n",
        "actor_critic.value_head.to(device=device, dtype=dtype)\n",
        "\n",
        "# 参考策略 π_ref（冻结，放 CPU 省显存）\n",
        "ref_device = \"cpu\"\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32).to(ref_device)\n",
        "ref_model.eval()\n",
        "for p in ref_model.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "print(\"ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 手写 LoRA 注入（不使用 peft）\n",
        "\n",
        "对一个 Linear 权重 $W$，LoRA 训练的是低秩增量：\n",
        "\n",
        "$$\n",
        "W' = W + \\Delta W, \\quad \\Delta W = \\alpha/r \\cdot BA\n",
        "$$\n",
        "\n",
        "其中 $A\\in\\mathbb{R}^{r\\times d}$，$B\\in\\mathbb{R}^{k\\times r}$ 是可训练参数，$W$ 冻结。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LoRALinear(torch.nn.Module):\n",
        "    def __init__(self, base: torch.nn.Linear, r: int = 8, alpha: int = 16, dropout: float = 0.05):\n",
        "        super().__init__()\n",
        "        if not isinstance(base, torch.nn.Linear):\n",
        "            raise TypeError(\"LoRALinear only supports torch.nn.Linear\")\n",
        "\n",
        "        self.base = base\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / r\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        in_features = base.in_features\n",
        "        out_features = base.out_features\n",
        "\n",
        "        self.lora_A = torch.nn.Parameter(torch.empty((r, in_features), dtype=base.weight.dtype, device=base.weight.device))\n",
        "        self.lora_B = torch.nn.Parameter(torch.zeros((out_features, r), dtype=base.weight.dtype, device=base.weight.device))\n",
        "\n",
        "        torch.nn.init.kaiming_uniform_(self.lora_A, a=5**0.5)\n",
        "\n",
        "        self.base.weight.requires_grad_(False)\n",
        "        if self.base.bias is not None:\n",
        "            self.base.bias.requires_grad_(False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        base_out = self.base(x)\n",
        "        lora_out = (self.dropout(x) @ self.lora_A.t()) @ self.lora_B.t()\n",
        "        return base_out + lora_out * self.scaling\n",
        "\n",
        "def _get_parent_module(root: torch.nn.Module, module_name: str) -> Tuple[torch.nn.Module, str]:\n",
        "    parts = module_name.split(\".\")\n",
        "    parent = root\n",
        "    for p in parts[:-1]:\n",
        "        parent = getattr(parent, p)\n",
        "    return parent, parts[-1]\n",
        "\n",
        "def inject_lora(\n",
        "    m: torch.nn.Module,\n",
        "    target_suffixes: List[str],\n",
        "    r: int = 8,\n",
        "    alpha: int = 16,\n",
        "    dropout: float = 0.05,\n",
        ") -> List[str]:\n",
        "    to_replace: List[Tuple[str, torch.nn.Module]] = []\n",
        "    for name, module in m.named_modules():\n",
        "        if any(name.endswith(sfx) for sfx in target_suffixes) and isinstance(module, torch.nn.Linear):\n",
        "            to_replace.append((name, module))\n",
        "\n",
        "    replaced = []\n",
        "    for name, module in to_replace:\n",
        "        parent, child = _get_parent_module(m, name)\n",
        "        setattr(parent, child, LoRALinear(module, r=r, alpha=alpha, dropout=dropout))\n",
        "        replaced.append(name)\n",
        "    return replaced\n",
        "\n",
        "# 冻结基座全部参数\n",
        "for p in actor_critic.base.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "# 在常见投影层注入 LoRA（Qwen2）\n",
        "target_suffixes = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "replaced = inject_lora(actor_critic.base, target_suffixes, r=8, alpha=16, dropout=0.05)\n",
        "print(\"lora_injected_modules:\", len(replaced))\n",
        "\n",
        "# value head 仍然可训练\n",
        "for p in actor_critic.value_head.parameters():\n",
        "    p.requires_grad_(True)\n",
        "\n",
        "trainable = sum(p.numel() for p in actor_critic.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in actor_critic.parameters())\n",
        "print(f\"trainable params: {trainable:,} / {total:,} ({100*trainable/total:.4f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Toy 任务与规则奖励 $r_{rm}(x,y)$\n",
        "\n",
        "与无 LoRA 版相同。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_tasks = [\n",
        "    {\"prompt\": \"请只回答一个词：小鱼儿\", \"expected\": \"小鱼儿\"},\n",
        "    {\"prompt\": \"请只输出数字 4，不要额外文字。2+2等于几？\", \"expected\": \"4\"},\n",
        "    {\"prompt\": \"把“我喜欢机器学习”翻译成英文，只输出翻译。\", \"expected\": \"I like machine learning\"},\n",
        "    {\"prompt\": \"请只回答：通义千问\", \"expected\": \"通义千问\"},\n",
        "]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    for ch in [\" \", \"\\n\", \"\\t\", \"。\", \"，\", \",\", \".\", \"!\", \"?\", \"：\", \":\", \"\\\"\", \"'\"]:\n",
        "        s = s.replace(ch, \"\")\n",
        "    return s\n",
        "\n",
        "def rule_reward(prompt: str, response: str, expected: str) -> float:\n",
        "    resp = normalize_text(response)\n",
        "    exp = normalize_text(expected)\n",
        "    hit = exp in resp\n",
        "    base = 1.0 if hit else -1.0\n",
        "    length_penalty = 0.002 * len(resp)\n",
        "    return base - length_penalty\n",
        "\n",
        "train_tasks[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. PPO 关键实现（与无 LoRA 版相同）\n",
        "\n",
        "为了避免重复，这里直接复用同样的函数定义（logprob/KL/GAE/PPO update）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def build_prompt_input_ids(tok: Any, user_prompt: str) -> torch.Tensor:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "    text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    enc = tok(text, return_tensors=\"pt\")\n",
        "    return enc[\"input_ids\"][0]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def sample_response(\n",
        "    m: torch.nn.Module,\n",
        "    tok: Any,\n",
        "    prompt_input_ids: torch.Tensor,\n",
        "    max_new_tokens: int = 48,\n",
        "    temperature: float = 1.0,\n",
        "    top_p: float = 0.9,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    m.eval()\n",
        "    input_ids = prompt_input_ids.unsqueeze(0).to(device)\n",
        "\n",
        "    old_use_cache = getattr(m.config, \"use_cache\", True)\n",
        "    m.config.use_cache = True\n",
        "    out = m.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "    )\n",
        "    m.config.use_cache = old_use_cache\n",
        "\n",
        "    full_ids = out[0].detach().cpu()\n",
        "    response_ids = full_ids[prompt_input_ids.numel() :]\n",
        "    return full_ids, response_ids\n",
        "\n",
        "def action_logprobs_from_logits(\n",
        "    logits: torch.Tensor,\n",
        "    input_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    positions = torch.arange(prompt_len, prompt_len + response_len, device=logits.device)\n",
        "    logp_positions = positions - 1\n",
        "    action_ids = input_ids[0, positions]\n",
        "    action_logp = log_probs[0, logp_positions, :].gather(dim=-1, index=action_ids.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    step_log_probs = log_probs[0, logp_positions, :]\n",
        "    entropy = -(step_log_probs.exp() * step_log_probs).sum(dim=-1)\n",
        "    return action_logp, entropy\n",
        "\n",
        "def get_policy_logp_value_entropy(\n",
        "    ac: ActorCritic,\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        "    use_grad: bool,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    ctx = torch.enable_grad() if use_grad else torch.inference_mode()\n",
        "    with ctx:\n",
        "        input_ids = full_ids.unsqueeze(0).to(device)\n",
        "        attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
        "        logits, values_all = ac(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        action_logp, entropy = action_logprobs_from_logits(logits, input_ids, prompt_len, response_len)\n",
        "        positions = torch.arange(prompt_len, prompt_len + response_len, device=device)\n",
        "        value_positions = positions - 1\n",
        "        values = values_all[0, value_positions]\n",
        "        return action_logp, values, entropy\n",
        "\n",
        "@torch.inference_mode()\n",
        "def get_ref_logp(\n",
        "    ref: torch.nn.Module,\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        ") -> torch.Tensor:\n",
        "    input_ids = full_ids.unsqueeze(0).to(ref_device)\n",
        "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
        "    logits = ref(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, return_dict=True).logits\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    positions = torch.arange(prompt_len, prompt_len + response_len, device=logits.device)\n",
        "    logp_positions = positions - 1\n",
        "    action_ids = input_ids[0, positions]\n",
        "    logp = log_probs[0, logp_positions, :].gather(dim=-1, index=action_ids.unsqueeze(-1)).squeeze(-1)\n",
        "    return logp.to(device)\n",
        "\n",
        "def compute_gae(rewards: torch.Tensor, values: torch.Tensor, gamma: float, gae_lambda: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    T = rewards.shape[0]\n",
        "    advantages = torch.zeros_like(rewards)\n",
        "    last_gae = torch.zeros((), device=rewards.device, dtype=rewards.dtype)\n",
        "    for t in reversed(range(T)):\n",
        "        next_value = values[t + 1] if t < T - 1 else torch.zeros((), device=values.device, dtype=values.dtype)\n",
        "        delta = rewards[t] + gamma * next_value - values[t]\n",
        "        last_gae = delta + gamma * gae_lambda * last_gae\n",
        "        advantages[t] = last_gae\n",
        "    returns = advantages + values\n",
        "    return advantages, returns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. PPO 训练循环（LoRA 版）\n",
        "\n",
        "与无 LoRA 版相同，但：\n",
        "\n",
        "- 学习率可以相对大一些\n",
        "- 优化器只包含 `requires_grad=True` 的参数（LoRA + value head）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_steps = 40\n",
        "max_new_tokens = 48\n",
        "\n",
        "kl_coef = 0.05\n",
        "gamma = 1.0\n",
        "gae_lambda = 0.95\n",
        "\n",
        "clip_eps = 0.2\n",
        "value_clip_eps = 0.2\n",
        "vf_coef = 0.5\n",
        "ent_coef = 0.0\n",
        "ppo_epochs = 2\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "lr = 1e-4\n",
        "optimizer = torch.optim.AdamW([p for p in actor_critic.parameters() if p.requires_grad], lr=lr)\n",
        "\n",
        "use_amp = device == \"cuda\"\n",
        "use_bf16 = use_amp and torch.cuda.is_bf16_supported()\n",
        "autocast_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "autocast_device_type = \"cuda\" if use_amp else \"cpu\"\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp and not use_bf16)\n",
        "\n",
        "def ppo_update_one_trajectory(\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        "    logpi_old: torch.Tensor,\n",
        "    values_old: torch.Tensor,\n",
        "    advantages: torch.Tensor,\n",
        "    returns: torch.Tensor,\n",
        ") -> Dict[str, float]:\n",
        "    actor_critic.train()\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std().clamp_min(1e-8))\n",
        "\n",
        "    metrics: Dict[str, float] = {}\n",
        "    for epoch in range(ppo_epochs):\n",
        "        with torch.autocast(device_type=autocast_device_type, dtype=autocast_dtype, enabled=use_amp):\n",
        "            logpi, values, entropy = get_policy_logp_value_entropy(\n",
        "                actor_critic, full_ids, prompt_len, response_len, use_grad=True\n",
        "            )\n",
        "\n",
        "            ratio = torch.exp(logpi - logpi_old)\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            values_clipped = values_old + (values - values_old).clamp(-value_clip_eps, value_clip_eps)\n",
        "            v_loss1 = (values - returns) ** 2\n",
        "            v_loss2 = (values_clipped - returns) ** 2\n",
        "            value_loss = 0.5 * torch.max(v_loss1, v_loss2).mean()\n",
        "\n",
        "            entropy_loss = -entropy.mean()\n",
        "            loss = policy_loss + vf_coef * value_loss + ent_coef * entropy_loss\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if scaler.is_enabled():\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_([p for p in actor_critic.parameters() if p.requires_grad], max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            clip_grad_norm_([p for p in actor_critic.parameters() if p.requires_grad], max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        approx_kl = (logpi_old - logpi).mean().detach()\n",
        "        clip_frac = ((ratio - 1.0).abs() > clip_eps).float().mean().detach()\n",
        "\n",
        "        metrics = {\n",
        "            \"loss\": float(loss.detach().cpu().item()),\n",
        "            \"policy_loss\": float(policy_loss.detach().cpu().item()),\n",
        "            \"value_loss\": float(value_loss.detach().cpu().item()),\n",
        "            \"entropy\": float(entropy.mean().detach().cpu().item()),\n",
        "            \"approx_kl\": float(approx_kl.cpu().item()),\n",
        "            \"clip_frac\": float(clip_frac.cpu().item()),\n",
        "        }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "@torch.inference_mode()\n",
        "def rollout_one(prompt: str, expected: str) -> Dict[str, Any]:\n",
        "    prompt_ids = build_prompt_input_ids(tokenizer, prompt)\n",
        "    full_ids, response_ids = sample_response(actor_critic.base, tokenizer, prompt_ids, max_new_tokens=max_new_tokens)\n",
        "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "    prompt_len = prompt_ids.numel()\n",
        "    response_len = int(response_ids.numel())\n",
        "    if response_len == 0:\n",
        "        return {\"skip\": True, \"prompt\": prompt, \"response\": response_text}\n",
        "\n",
        "    rm_reward = rule_reward(prompt, response_text, expected)\n",
        "\n",
        "    logpi_old, values_old, _ = get_policy_logp_value_entropy(\n",
        "        actor_critic, full_ids, prompt_len, response_len, use_grad=False\n",
        "    )\n",
        "    logpi_ref = get_ref_logp(ref_model, full_ids, prompt_len, response_len)\n",
        "\n",
        "    kl = logpi_old - logpi_ref\n",
        "    rewards = -kl_coef * kl\n",
        "    rewards[-1] = rewards[-1] + torch.tensor(rm_reward, device=device, dtype=rewards.dtype)\n",
        "\n",
        "    advantages, returns = compute_gae(rewards, values_old, gamma=gamma, gae_lambda=gae_lambda)\n",
        "\n",
        "    return {\n",
        "        \"skip\": False,\n",
        "        \"prompt\": prompt,\n",
        "        \"expected\": expected,\n",
        "        \"response\": response_text,\n",
        "        \"full_ids\": full_ids,\n",
        "        \"prompt_len\": prompt_len,\n",
        "        \"response_len\": response_len,\n",
        "        \"rm_reward\": float(rm_reward),\n",
        "        \"kl_mean\": float(kl.mean().detach().cpu().item()),\n",
        "        \"logpi_old\": logpi_old,\n",
        "        \"values_old\": values_old,\n",
        "        \"advantages\": advantages.detach(),\n",
        "        \"returns\": returns.detach(),\n",
        "    }\n",
        "\n",
        "print(\"ready for training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for step in range(train_steps):\n",
        "    task = random.choice(train_tasks)\n",
        "    rollout = rollout_one(task[\"prompt\"], task[\"expected\"])\n",
        "    if rollout[\"skip\"]:\n",
        "        print(f\"step={step} skip(empty response)\")\n",
        "        continue\n",
        "\n",
        "    metrics = ppo_update_one_trajectory(\n",
        "        full_ids=rollout[\"full_ids\"],\n",
        "        prompt_len=rollout[\"prompt_len\"],\n",
        "        response_len=rollout[\"response_len\"],\n",
        "        logpi_old=rollout[\"logpi_old\"],\n",
        "        values_old=rollout[\"values_old\"],\n",
        "        advantages=rollout[\"advantages\"],\n",
        "        returns=rollout[\"returns\"],\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"step={step} rm_reward={rollout['rm_reward']:.3f} kl_mean={rollout['kl_mean']:.3f} \"\n",
        "        f\"loss={metrics['loss']:.4f} policy={metrics['policy_loss']:.4f} value={metrics['value_loss']:.4f} \"\n",
        "        f\"approx_kl={metrics['approx_kl']:.4f} clip_frac={metrics['clip_frac']:.3f}\"\n",
        "    )\n",
        "    print(\"prompt:\", rollout[\"prompt\"])\n",
        "    print(\"response:\", rollout[\"response\"])\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 简单验证\n",
        "\n",
        "训练后再看一眼输出。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def chat(prompt: str, max_new_tokens: int = 64) -> str:\n",
        "    prompt_ids = build_prompt_input_ids(tokenizer, prompt)\n",
        "    full_ids, response_ids = sample_response(actor_critic.base, tokenizer, prompt_ids, max_new_tokens=max_new_tokens, temperature=0.7, top_p=0.9)\n",
        "    return tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "for t in train_tasks:\n",
        "    print(\"Q:\", t[\"prompt\"])\n",
        "    print(\"A:\", chat(t[\"prompt\"]))\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
