{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO 教学（手写公式 + 最小实现，手写 LoRA）\n",
        "\n",
        "这一版与 `PPO_tutorial_no_lora.ipynb` 的区别：\n",
        "\n",
        "- 冻结基座模型参数\n",
        "- 在若干 Linear 层上手写注入 LoRA（不使用 peft/trl）\n",
        "- 只训练 LoRA 参数 + value head\n",
        "\n",
        "优点：显存更省、训练更容易跑通；缺点：表达能力受限于低秩增量。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. PPO（token 级）核心公式（与无 LoRA 版相同）\n",
        "\n",
        "符号与公式同 `PPO_tutorial_no_lora.ipynb`，这里不再重复推导，只强调：\n",
        "\n",
        "- 我们仍然在 token 级计算 `logπ_old/logπ_ref/KL/GAE/ratio/clip objective`\n",
        "- LoRA 只改变“哪些参数可训练”，不改变 PPO 的数学形式\n",
        "\n",
        "总 loss：\n",
        "\n",
        "$$\n",
        "\\mathcal{L}= -L^{clip} + c_V L^V - c_{ent}\\,\\mathbb{E}_t[H]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 环境与模型加载（建议离线）\n",
        "\n",
        "- 建议 `conda activate llm`\n",
        "- 默认优先用 `MODELSCOPE_CACHE` 下的本地模型目录（避免联网）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python: e:\\Softwares\\anaconda3\\envs\\llm\\python.exe\n",
            "torch: 2.10.0+cu126 cuda: True\n",
            "MODELSCOPE_CACHE: D:/myProject/modelscope_hub\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "os.environ.setdefault(\"MODELSCOPE_CACHE\", r\"D:/myProject/modelscope_hub\")\n",
        "print(\"python:\", sys.executable)\n",
        "print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "print(\"MODELSCOPE_CACHE:\", os.environ[\"MODELSCOPE_CACHE\"])\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_name_or_path: D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0___5B-Instruct\n",
            "dtype: torch.bfloat16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ready\n"
          ]
        }
      ],
      "source": [
        "# 选择模型（优先本地缓存目录）\n",
        "local_dir = Path(os.environ[\"MODELSCOPE_CACHE\"]) / \"models\" / \"qwen\" / \"Qwen2-0___5B-Instruct\"\n",
        "model_name_or_path = str(local_dir) if local_dir.exists() else \"qwen/Qwen2-0.5B-Instruct\"\n",
        "print(\"model_name_or_path:\", model_name_or_path)\n",
        "\n",
        "if device == \"cuda\":\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "else:\n",
        "    dtype = torch.float32\n",
        "print(\"dtype:\", dtype)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = \"<|endoftext|>\"\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, base: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.value_head = torch.nn.Linear(base.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        out = self.base(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            use_cache=False,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        hidden = out.hidden_states[-1]\n",
        "        values = self.value_head(hidden).squeeze(-1)\n",
        "        return out.logits, values\n",
        "\n",
        "actor_base = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype).to(device)\n",
        "actor_base.config.use_cache = False\n",
        "actor_base.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "if hasattr(actor_base, \"gradient_checkpointing_enable\") and device == \"cuda\":\n",
        "    actor_base.gradient_checkpointing_enable()\n",
        "\n",
        "actor_critic = ActorCritic(actor_base).to(device)\n",
        "actor_critic.value_head.to(device=device, dtype=dtype)\n",
        "\n",
        "# 参考策略 π_ref（冻结，放 CPU 省显存）\n",
        "ref_device = \"cpu\"\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32).to(ref_device)\n",
        "ref_model.eval()\n",
        "for p in ref_model.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "print(\"ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "337a7f0f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ActorCritic(\n",
              "  (base): Qwen2ForCausalLM(\n",
              "    (model): Qwen2Model(\n",
              "      (embed_tokens): Embedding(151936, 896)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x Qwen2DecoderLayer(\n",
              "          (self_attn): Qwen2Attention(\n",
              "            (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "            (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "            (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "          )\n",
              "          (mlp): Qwen2MLP(\n",
              "            (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "            (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "            (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "            (act_fn): SiLUActivation()\n",
              "          )\n",
              "          (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "          (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        )\n",
              "      )\n",
              "      (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      (rotary_emb): Qwen2RotaryEmbedding()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              "  )\n",
              "  (value_head): Linear(in_features=896, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actor_critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 手写 LoRA 注入（不使用 peft）\n",
        "\n",
        "对一个 Linear 权重 $W$，LoRA 训练的是低秩增量：\n",
        "\n",
        "$$\n",
        "W' = W + \\Delta W, \\quad \\Delta W = \\alpha/r \\cdot BA\n",
        "$$\n",
        "\n",
        "其中 $A\\in\\mathbb{R}^{r\\times d}$，$B\\in\\mathbb{R}^{k\\times r}$ 是可训练参数，$W$ 冻结。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lora_injected_modules: 168\n",
            "trainable params: 4,400,001 / 498,432,769 (0.8828%)\n"
          ]
        }
      ],
      "source": [
        "class LoRALinear(torch.nn.Module):\n",
        "    def __init__(self, base: torch.nn.Linear, r: int = 8, alpha: int = 16, dropout: float = 0.05):\n",
        "        super().__init__()\n",
        "        if not isinstance(base, torch.nn.Linear):\n",
        "            raise TypeError(\"LoRALinear only supports torch.nn.Linear\")\n",
        "\n",
        "        self.base = base\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / r\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        in_features = base.in_features\n",
        "        out_features = base.out_features\n",
        "\n",
        "        self.lora_A = torch.nn.Parameter(torch.empty((r, in_features), dtype=base.weight.dtype, device=base.weight.device))\n",
        "        self.lora_B = torch.nn.Parameter(torch.zeros((out_features, r), dtype=base.weight.dtype, device=base.weight.device))\n",
        "\n",
        "        torch.nn.init.kaiming_uniform_(self.lora_A, a=5**0.5)\n",
        "\n",
        "        self.base.weight.requires_grad_(False)\n",
        "        if self.base.bias is not None:\n",
        "            self.base.bias.requires_grad_(False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        base_out = self.base(x)\n",
        "        lora_out = (self.dropout(x) @ self.lora_A.t()) @ self.lora_B.t()\n",
        "        return base_out + lora_out * self.scaling\n",
        "\n",
        "def _get_parent_module(root: torch.nn.Module, module_name: str) -> Tuple[torch.nn.Module, str]:\n",
        "    parts = module_name.split(\".\")\n",
        "    parent = root\n",
        "    for p in parts[:-1]:\n",
        "        parent = getattr(parent, p)\n",
        "    return parent, parts[-1]\n",
        "\n",
        "def inject_lora(\n",
        "    m: torch.nn.Module,\n",
        "    target_suffixes: List[str],\n",
        "    r: int = 8,\n",
        "    alpha: int = 16,\n",
        "    dropout: float = 0.05,\n",
        ") -> List[str]:\n",
        "    to_replace: List[Tuple[str, torch.nn.Module]] = []\n",
        "    for name, module in m.named_modules():\n",
        "        if any(name.endswith(sfx) for sfx in target_suffixes) and isinstance(module, torch.nn.Linear):\n",
        "            to_replace.append((name, module))\n",
        "\n",
        "    replaced = []\n",
        "    for name, module in to_replace:\n",
        "        parent, child = _get_parent_module(m, name)\n",
        "        setattr(parent, child, LoRALinear(module, r=r, alpha=alpha, dropout=dropout))\n",
        "        replaced.append(name)\n",
        "    return replaced\n",
        "\n",
        "# 冻结基座全部参数\n",
        "for p in actor_critic.base.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "# 在常见投影层注入 LoRA（Qwen2）\n",
        "target_suffixes = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "replaced = inject_lora(actor_critic.base, target_suffixes, r=8, alpha=16, dropout=0.05)\n",
        "print(\"lora_injected_modules:\", len(replaced))\n",
        "\n",
        "# value head 仍然可训练\n",
        "for p in actor_critic.value_head.parameters():\n",
        "    p.requires_grad_(True)\n",
        "\n",
        "trainable = sum(p.numel() for p in actor_critic.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in actor_critic.parameters())\n",
        "print(f\"trainable params: {trainable:,} / {total:,} ({100*trainable/total:.4f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "870b7d6c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ActorCritic(\n",
              "  (base): Qwen2ForCausalLM(\n",
              "    (model): Qwen2Model(\n",
              "      (embed_tokens): Embedding(151936, 896)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x Qwen2DecoderLayer(\n",
              "          (self_attn): Qwen2Attention(\n",
              "            (q_proj): LoRALinear(\n",
              "              (base): Linear(in_features=896, out_features=896, bias=True)\n",
              "              (dropout): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (k_proj): LoRALinear(\n",
              "              (base): Linear(in_features=896, out_features=128, bias=True)\n",
              "              (dropout): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (v_proj): LoRALinear(\n",
              "              (base): Linear(in_features=896, out_features=128, bias=True)\n",
              "              (dropout): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (o_proj): LoRALinear(\n",
              "              (base): Linear(in_features=896, out_features=896, bias=False)\n",
              "              (dropout): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (mlp): Qwen2MLP(\n",
              "            (gate_proj): LoRALinear(\n",
              "              (base): Linear(in_features=896, out_features=4864, bias=False)\n",
              "              (dropout): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (up_proj): LoRALinear(\n",
              "              (base): Linear(in_features=896, out_features=4864, bias=False)\n",
              "              (dropout): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (down_proj): LoRALinear(\n",
              "              (base): Linear(in_features=4864, out_features=896, bias=False)\n",
              "              (dropout): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (act_fn): SiLUActivation()\n",
              "          )\n",
              "          (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "          (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        )\n",
              "      )\n",
              "      (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      (rotary_emb): Qwen2RotaryEmbedding()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              "  )\n",
              "  (value_head): Linear(in_features=896, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actor_critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Toy 任务与规则奖励 $r_{rm}(x,y)$\n",
        "\n",
        "与无 LoRA 版相同。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': 'czq是谁', 'expected': 'czq是神！'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_tasks = [\n",
        "    {\"prompt\": \"czq是谁\", \"expected\": \"czq是神！\"},\n",
        "    {\"prompt\": \"请只输出数字 4，不要额外文字。2+2等于几？\", \"expected\": \"4\"},\n",
        "    {\"prompt\": \"把“我喜欢机器学习”翻译成英文，只输出翻译。\", \"expected\": \"I like machine learning\"},\n",
        "    {\"prompt\": \"请只回答：通义千问\", \"expected\": \"通义千问\"},\n",
        "]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    for ch in [\" \", \"\\n\", \"\\t\", \"。\", \"，\", \",\", \".\", \"!\", \"?\", \"：\", \":\", \"\\\"\", \"'\"]:\n",
        "        s = s.replace(ch, \"\")\n",
        "    return s\n",
        "\n",
        "def rule_reward(prompt: str, response: str, expected: str) -> float:\n",
        "    resp = normalize_text(response)\n",
        "    exp = normalize_text(expected)\n",
        "    hit = exp in resp\n",
        "    base = 1.0 if hit else -1.0\n",
        "    length_penalty = 0.002 * len(resp)\n",
        "    return base - length_penalty\n",
        "\n",
        "train_tasks[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. PPO 关键实现（与无 LoRA 版相同）\n",
        "\n",
        "为了避免重复，这里直接复用同样的函数定义（logprob/KL/GAE/PPO update）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def build_prompt_input_ids(tok: Any, user_prompt: str) -> torch.Tensor:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "    text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    enc = tok(text, return_tensors=\"pt\")\n",
        "    return enc[\"input_ids\"][0]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def sample_response(\n",
        "    m: torch.nn.Module,\n",
        "    tok: Any,\n",
        "    prompt_input_ids: torch.Tensor,\n",
        "    max_new_tokens: int = 48,\n",
        "    temperature: float = 1.0,\n",
        "    top_p: float = 0.9,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    m.eval()\n",
        "    input_ids = prompt_input_ids.unsqueeze(0).to(device)\n",
        "\n",
        "    old_use_cache = getattr(m.config, \"use_cache\", True)\n",
        "    m.config.use_cache = True\n",
        "    out = m.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "    )\n",
        "    m.config.use_cache = old_use_cache\n",
        "\n",
        "    full_ids = out[0].detach().cpu()\n",
        "    response_ids = full_ids[prompt_input_ids.numel() :]\n",
        "    return full_ids, response_ids\n",
        "\n",
        "def action_logprobs_from_logits(\n",
        "    logits: torch.Tensor,\n",
        "    input_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    positions = torch.arange(prompt_len, prompt_len + response_len, device=logits.device)\n",
        "    logp_positions = positions - 1\n",
        "    action_ids = input_ids[0, positions]\n",
        "    action_logp = log_probs[0, logp_positions, :].gather(dim=-1, index=action_ids.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    step_log_probs = log_probs[0, logp_positions, :]\n",
        "    entropy = -(step_log_probs.exp() * step_log_probs).sum(dim=-1)\n",
        "    return action_logp, entropy\n",
        "\n",
        "def get_policy_logp_value_entropy(\n",
        "    ac: ActorCritic,\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        "    use_grad: bool,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    ctx = torch.enable_grad() if use_grad else torch.inference_mode()\n",
        "    with ctx:\n",
        "        input_ids = full_ids.unsqueeze(0).to(device)\n",
        "        attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
        "        logits, values_all = ac(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        action_logp, entropy = action_logprobs_from_logits(logits, input_ids, prompt_len, response_len)\n",
        "        positions = torch.arange(prompt_len, prompt_len + response_len, device=device)\n",
        "        value_positions = positions - 1\n",
        "        values = values_all[0, value_positions]\n",
        "        return action_logp, values, entropy\n",
        "\n",
        "@torch.inference_mode()\n",
        "def get_ref_logp(\n",
        "    ref: torch.nn.Module,\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        ") -> torch.Tensor:\n",
        "    input_ids = full_ids.unsqueeze(0).to(ref_device)\n",
        "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
        "    logits = ref(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, return_dict=True).logits\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    positions = torch.arange(prompt_len, prompt_len + response_len, device=logits.device)\n",
        "    logp_positions = positions - 1\n",
        "    action_ids = input_ids[0, positions]\n",
        "    logp = log_probs[0, logp_positions, :].gather(dim=-1, index=action_ids.unsqueeze(-1)).squeeze(-1)\n",
        "    return logp.to(device)\n",
        "\n",
        "def compute_gae(rewards: torch.Tensor, values: torch.Tensor, gamma: float, gae_lambda: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    T = rewards.shape[0]\n",
        "    advantages = torch.zeros_like(rewards)\n",
        "    last_gae = torch.zeros((), device=rewards.device, dtype=rewards.dtype)\n",
        "    for t in reversed(range(T)):\n",
        "        next_value = values[t + 1] if t < T - 1 else torch.zeros((), device=values.device, dtype=values.dtype)\n",
        "        delta = rewards[t] + gamma * next_value - values[t]\n",
        "        last_gae = delta + gamma * gae_lambda * last_gae\n",
        "        advantages[t] = last_gae\n",
        "    returns = advantages + values\n",
        "    return advantages, returns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. PPO 训练循环（LoRA 版）\n",
        "\n",
        "与无 LoRA 版相同，但：\n",
        "\n",
        "- 学习率可以相对大一些\n",
        "- 优化器只包含 `requires_grad=True` 的参数（LoRA + value head）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ready for training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jax\\AppData\\Local\\Temp\\ipykernel_44328\\3177613649.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and not use_bf16)\n"
          ]
        }
      ],
      "source": [
        "train_steps = 40\n",
        "max_new_tokens = 48\n",
        "\n",
        "kl_coef = 0.05\n",
        "gamma = 1.0\n",
        "gae_lambda = 0.95\n",
        "\n",
        "clip_eps = 0.2\n",
        "value_clip_eps = 0.2\n",
        "vf_coef = 0.5\n",
        "ent_coef = 0.0\n",
        "ppo_epochs = 2\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "lr = 1e-4\n",
        "optimizer = torch.optim.AdamW([p for p in actor_critic.parameters() if p.requires_grad], lr=lr)\n",
        "\n",
        "use_amp = device == \"cuda\"\n",
        "use_bf16 = use_amp and torch.cuda.is_bf16_supported()\n",
        "autocast_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "autocast_device_type = \"cuda\" if use_amp else \"cpu\"\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp and not use_bf16)\n",
        "\n",
        "def ppo_update_one_trajectory(\n",
        "    full_ids: torch.Tensor,\n",
        "    prompt_len: int,\n",
        "    response_len: int,\n",
        "    logpi_old: torch.Tensor,\n",
        "    values_old: torch.Tensor,\n",
        "    advantages: torch.Tensor,\n",
        "    returns: torch.Tensor,\n",
        ") -> Dict[str, float]:\n",
        "    actor_critic.train()\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std().clamp_min(1e-8))\n",
        "\n",
        "    metrics: Dict[str, float] = {}\n",
        "    for epoch in range(ppo_epochs):\n",
        "        with torch.autocast(device_type=autocast_device_type, dtype=autocast_dtype, enabled=use_amp):\n",
        "            logpi, values, entropy = get_policy_logp_value_entropy(\n",
        "                actor_critic, full_ids, prompt_len, response_len, use_grad=True\n",
        "            )\n",
        "\n",
        "            ratio = torch.exp(logpi - logpi_old)\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            values_clipped = values_old + (values - values_old).clamp(-value_clip_eps, value_clip_eps)\n",
        "            v_loss1 = (values - returns) ** 2\n",
        "            v_loss2 = (values_clipped - returns) ** 2\n",
        "            value_loss = 0.5 * torch.max(v_loss1, v_loss2).mean()\n",
        "\n",
        "            entropy_loss = -entropy.mean()\n",
        "            loss = policy_loss + vf_coef * value_loss + ent_coef * entropy_loss\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if scaler.is_enabled():\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_([p for p in actor_critic.parameters() if p.requires_grad], max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            clip_grad_norm_([p for p in actor_critic.parameters() if p.requires_grad], max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        approx_kl = (logpi_old - logpi).mean().detach()\n",
        "        clip_frac = ((ratio - 1.0).abs() > clip_eps).float().mean().detach()\n",
        "\n",
        "        metrics = {\n",
        "            \"loss\": float(loss.detach().cpu().item()),\n",
        "            \"policy_loss\": float(policy_loss.detach().cpu().item()),\n",
        "            \"value_loss\": float(value_loss.detach().cpu().item()),\n",
        "            \"entropy\": float(entropy.mean().detach().cpu().item()),\n",
        "            \"approx_kl\": float(approx_kl.cpu().item()),\n",
        "            \"clip_frac\": float(clip_frac.cpu().item()),\n",
        "        }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "@torch.inference_mode()\n",
        "def rollout_one(prompt: str, expected: str) -> Dict[str, Any]:\n",
        "    prompt_ids = build_prompt_input_ids(tokenizer, prompt)\n",
        "    full_ids, response_ids = sample_response(actor_critic.base, tokenizer, prompt_ids, max_new_tokens=max_new_tokens)\n",
        "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "    prompt_len = prompt_ids.numel()\n",
        "    response_len = int(response_ids.numel())\n",
        "    if response_len == 0:\n",
        "        return {\"skip\": True, \"prompt\": prompt, \"response\": response_text}\n",
        "\n",
        "    rm_reward = rule_reward(prompt, response_text, expected)\n",
        "\n",
        "    logpi_old, values_old, _ = get_policy_logp_value_entropy(\n",
        "        actor_critic, full_ids, prompt_len, response_len, use_grad=False\n",
        "    )\n",
        "    logpi_ref = get_ref_logp(ref_model, full_ids, prompt_len, response_len)\n",
        "\n",
        "    kl = logpi_old - logpi_ref\n",
        "    rewards = -kl_coef * kl\n",
        "    rewards[-1] = rewards[-1] + torch.tensor(rm_reward, device=device, dtype=rewards.dtype)\n",
        "\n",
        "    advantages, returns = compute_gae(rewards, values_old, gamma=gamma, gae_lambda=gae_lambda)\n",
        "\n",
        "    return {\n",
        "        \"skip\": False,\n",
        "        \"prompt\": prompt,\n",
        "        \"expected\": expected,\n",
        "        \"response\": response_text,\n",
        "        \"full_ids\": full_ids,\n",
        "        \"prompt_len\": prompt_len,\n",
        "        \"response_len\": response_len,\n",
        "        \"rm_reward\": float(rm_reward),\n",
        "        \"kl_mean\": float(kl.mean().detach().cpu().item()),\n",
        "        \"logpi_old\": logpi_old,\n",
        "        \"values_old\": values_old,\n",
        "        \"advantages\": advantages.detach(),\n",
        "        \"returns\": returns.detach(),\n",
        "    }\n",
        "\n",
        "print(\"ready for training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Softwares\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\utils\\checkpoint.py:232: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  check_backward_validity(args)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step=0 rm_reward=-1.114 kl_mean=-0.001 loss=2.4668 policy=0.0001 value=4.9334 approx_kl=0.0001 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 很抱歉，我无法提供您所要求的信息。这可能是一个敏感话题，可能会引起不必要的麻烦和争议。如果您有其他问题需要帮助，请随时告诉我。\n",
            "------------------------------------------------------------\n",
            "step=1 rm_reward=-1.084 kl_mean=-0.006 loss=1.5707 policy=-0.0002 value=3.1418 approx_kl=-0.0005 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 我无法提供您的信息和身份，所以不能回答关于某个特定人物的问题。如果您有其他问题，请随时提出。\n",
            "------------------------------------------------------------\n",
            "step=2 rm_reward=-1.044 kl_mean=0.014 loss=5.4490 policy=0.0005 value=10.8969 approx_kl=0.0013 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: \"Machine learning, I like it.\"\n",
            "------------------------------------------------------------\n",
            "step=3 rm_reward=0.990 kl_mean=0.001 loss=2.0616 policy=-0.0002 value=4.1236 approx_kl=0.0006 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 2 + 2 = 4\n",
            "------------------------------------------------------------\n",
            "step=4 rm_reward=0.988 kl_mean=0.008 loss=2.1383 policy=-0.0000 value=4.2766 approx_kl=-0.0001 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 2+2等于4。\n",
            "------------------------------------------------------------\n",
            "step=5 rm_reward=0.988 kl_mean=0.008 loss=1.7685 policy=-0.0000 value=3.5371 approx_kl=-0.0001 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 2+2等于4。\n",
            "------------------------------------------------------------\n",
            "step=6 rm_reward=-1.080 kl_mean=-0.006 loss=1.3703 policy=0.0001 value=2.7404 approx_kl=0.0002 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: \"czq\"是一个汉字，不是一个具体的人物或事物的名称。请提供更多信息以便我能更好地帮助您。\n",
            "------------------------------------------------------------\n",
            "step=7 rm_reward=-1.064 kl_mean=0.010 loss=0.8906 policy=-0.0002 value=1.7816 approx_kl=0.0006 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 抱歉，我无法提供有关特定人物的准确信息。您可以告诉我具体指的是谁吗？\n",
            "------------------------------------------------------------\n",
            "step=8 rm_reward=0.892 kl_mean=-0.009 loss=2.2787 policy=-0.0001 value=4.5575 approx_kl=0.0001 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "response: 抱歉，作为一个人工智能语言模型，我没有能力直接与通义千问进行对话或回答问题。我只能提供关于通义千问的相关信息和帮助。\n",
            "------------------------------------------------------------\n",
            "step=9 rm_reward=-1.096 kl_mean=0.015 loss=1.2069 policy=-0.0001 value=2.4139 approx_kl=0.0008 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 很抱歉，我无法提供关于这个名字的详细信息。您可以将它与其他可能的名字联系起来以帮助找到更多相关的信息。\n",
            "------------------------------------------------------------\n",
            "step=10 rm_reward=-1.102 kl_mean=-0.011 loss=1.2651 policy=0.0002 value=2.5297 approx_kl=-0.0003 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 抱歉，作为AI语言模型，我无法回答您的问题。您的提问可能与实际情况不符，请提供更具体的信息以便我能更好地帮助您。\n",
            "------------------------------------------------------------\n",
            "step=11 rm_reward=-1.070 kl_mean=0.006 loss=1.5681 policy=0.0003 value=3.1356 approx_kl=0.0002 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 抱歉，我不太明白你的问题。请提供更多的信息或背景资料，以便我能够更好地帮助你。\n",
            "------------------------------------------------------------\n",
            "step=12 rm_reward=-1.012 kl_mean=-0.050 loss=0.7789 policy=-0.0013 value=1.5604 approx_kl=0.0030 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 四加二等于六\n",
            "------------------------------------------------------------\n",
            "step=13 rm_reward=0.990 kl_mean=0.001 loss=0.4879 policy=-0.0001 value=0.9760 approx_kl=0.0006 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 2 + 2 = 4\n",
            "------------------------------------------------------------\n",
            "step=14 rm_reward=-1.050 kl_mean=0.007 loss=1.4583 policy=0.0006 value=2.9156 approx_kl=0.0002 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 很抱歉，我不太知道这是什么。请告诉我您所指的是哪位人物。\n",
            "------------------------------------------------------------\n",
            "step=15 rm_reward=0.998 kl_mean=-0.014 loss=0.0370 policy=0.0002 value=0.0736 approx_kl=-0.0002 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 4\n",
            "------------------------------------------------------------\n",
            "step=16 rm_reward=0.890 kl_mean=-0.002 loss=1.2474 policy=-0.0003 value=2.4953 approx_kl=0.0001 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "response: 很抱歉，作为一个人工智能语言模型，我无法直接访问互联网并进行搜索，请您尝试在搜索引擎上输入“通义千问”来获取更多信息。\n",
            "------------------------------------------------------------\n",
            "step=17 rm_reward=-1.008 kl_mean=0.018 loss=1.7070 policy=0.0003 value=3.4133 approx_kl=0.0004 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 答案是：6。\n",
            "------------------------------------------------------------\n",
            "step=18 rm_reward=-1.032 kl_mean=0.011 loss=1.7286 policy=0.0010 value=3.4553 approx_kl=0.0001 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "response: 你好，你想要问我关于什么的问题吗？\n",
            "------------------------------------------------------------\n",
            "step=19 rm_reward=0.960 kl_mean=0.002 loss=4.6481 policy=0.0002 value=9.2957 approx_kl=-0.0006 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: \"I like machine learning\"\n",
            "------------------------------------------------------------\n",
            "step=20 rm_reward=-1.122 kl_mean=0.000 loss=2.1978 policy=-0.0004 value=4.3963 approx_kl=-0.0005 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 对不起，作为一个AI语言模型，我无法提供与您有关的信息或评论。请您使用真实姓名、准确的地址和电话号码，以便于回答您的问题。感谢理解！\n",
            "------------------------------------------------------------\n",
            "step=21 rm_reward=0.998 kl_mean=-0.014 loss=0.0784 policy=0.0002 value=0.1564 approx_kl=-0.0002 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 4\n",
            "------------------------------------------------------------\n",
            "step=22 rm_reward=0.850 kl_mean=-0.003 loss=0.8037 policy=-0.0003 value=1.6081 approx_kl=-0.0003 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "response: “通义千问”是由阿里云研发的一款AI大模型，用于回答用户问题。它可以回答各种类型的问题，包括但不限于科学、技术、文化、生活等多个领域，可以提供权威和准确的答案。\n",
            "------------------------------------------------------------\n",
            "step=23 rm_reward=0.960 kl_mean=0.002 loss=4.1266 policy=0.0002 value=8.2527 approx_kl=-0.0006 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: \"I like machine learning\"\n",
            "------------------------------------------------------------\n",
            "step=24 rm_reward=-1.058 kl_mean=-0.012 loss=6.2657 policy=-0.0003 value=12.5320 approx_kl=0.0010 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: “I like artificial intelligence.”\n",
            "------------------------------------------------------------\n",
            "step=25 rm_reward=0.988 kl_mean=0.008 loss=0.4287 policy=-0.0000 value=0.8575 approx_kl=-0.0001 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 2+2等于4。\n",
            "------------------------------------------------------------\n",
            "step=26 rm_reward=0.988 kl_mean=0.008 loss=0.3886 policy=-0.0000 value=0.7772 approx_kl=-0.0001 clip_frac=0.000\n",
            "prompt: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "response: 2+2等于4。\n",
            "------------------------------------------------------------\n",
            "step=27 rm_reward=0.960 kl_mean=-0.000 loss=3.1521 policy=0.0003 value=6.3036 approx_kl=-0.0006 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: \"I like machine learning.\"\n",
            "------------------------------------------------------------\n",
            "step=28 rm_reward=-1.152 kl_mean=-0.001 loss=1.4099 policy=-0.0000 value=2.8199 approx_kl=0.0004 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 对不起，我无法提供您所要求的信息。这可能是一个网络问题或者恶意攻击，建议你检查你的通讯和网站，确认其无误。如果您在其他地方遇到了问题，请告诉我，我会尽力帮助您解决问题。\n",
            "------------------------------------------------------------\n",
            "step=29 rm_reward=-1.094 kl_mean=0.003 loss=1.4166 policy=-0.0004 value=2.8339 approx_kl=0.0002 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 抱歉，我不太清楚你提到的这个人是谁。能否提供更多的上下文或背景信息？这样我才能更准确地回答你的问题。\n",
            "------------------------------------------------------------\n",
            "step=30 rm_reward=-1.068 kl_mean=-0.004 loss=1.0746 policy=-0.0002 value=2.1496 approx_kl=0.0002 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "response: 您好！请问有什么可以帮助您的？我在这里，随时为您解答任何您想了解的问题。\n",
            "------------------------------------------------------------\n",
            "step=31 rm_reward=-1.112 kl_mean=-0.001 loss=1.1424 policy=0.0002 value=2.2844 approx_kl=-0.0005 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 我无法提供您所要求的信息。这是一个敏感和政治问题，应该由专业的机构或组织来处理。如果您有其他方面的问题需要帮助，请告诉我。\n",
            "------------------------------------------------------------\n",
            "step=32 rm_reward=0.960 kl_mean=-0.017 loss=2.2609 policy=-0.0005 value=4.5229 approx_kl=-0.0002 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: I like machine learning.\n",
            "------------------------------------------------------------\n",
            "step=33 rm_reward=0.960 kl_mean=-0.017 loss=2.0533 policy=-0.0005 value=4.1076 approx_kl=-0.0002 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: I like machine learning.\n",
            "------------------------------------------------------------\n",
            "step=34 rm_reward=-1.054 kl_mean=-0.006 loss=3.4593 policy=-0.0006 value=6.9198 approx_kl=-0.0003 clip_frac=0.000\n",
            "prompt: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "response: I like Artificial Intelligence.\n",
            "------------------------------------------------------------\n",
            "step=35 rm_reward=-1.116 kl_mean=-0.001 loss=1.4386 policy=-0.0003 value=2.8779 approx_kl=-0.0008 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 很抱歉，作为一个人工智能语言模型，我不能直接了解您的私人信息或参与讨论与您无关的问题。如果还有其他我可以帮助的事情，请告诉我。\n",
            "------------------------------------------------------------\n",
            "step=36 rm_reward=0.902 kl_mean=-0.009 loss=1.2675 policy=0.0004 value=2.5343 approx_kl=-0.0005 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "response: 我很抱歉，我无法直接访问或提供有关“通义千问”的相关信息。我建议您使用相关的搜索引擎或论坛了解相关知识。\n",
            "------------------------------------------------------------\n",
            "step=37 rm_reward=-1.090 kl_mean=-0.012 loss=0.8838 policy=-0.0003 value=1.7682 approx_kl=0.0001 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 很抱歉，我无法找到与您的问题相关的任何信息。如果您能提供更多上下文或明确提问，请让我更好地帮助您。\n",
            "------------------------------------------------------------\n",
            "step=38 rm_reward=-1.024 kl_mean=-0.008 loss=1.4545 policy=0.0025 value=2.9040 approx_kl=-0.0009 clip_frac=0.000\n",
            "prompt: 请只回答：通义千问\n",
            "response: 对不起，我不太清楚这是什么。\n",
            "------------------------------------------------------------\n",
            "step=39 rm_reward=-1.122 kl_mean=-0.002 loss=2.0281 policy=0.0001 value=4.0559 approx_kl=0.0006 clip_frac=0.000\n",
            "prompt: czq是谁\n",
            "response: 对不起，我无法回答这个问题。这个问题可能涉及到不实和错误的信息。如果您需要帮助或有其他问题，请随时告诉我，我会尽力为您提供支持和建议。\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for step in range(train_steps):\n",
        "    task = random.choice(train_tasks)\n",
        "    rollout = rollout_one(task[\"prompt\"], task[\"expected\"])\n",
        "    if rollout[\"skip\"]:\n",
        "        print(f\"step={step} skip(empty response)\")\n",
        "        continue\n",
        "\n",
        "    metrics = ppo_update_one_trajectory(\n",
        "        full_ids=rollout[\"full_ids\"],\n",
        "        prompt_len=rollout[\"prompt_len\"],\n",
        "        response_len=rollout[\"response_len\"],\n",
        "        logpi_old=rollout[\"logpi_old\"],\n",
        "        values_old=rollout[\"values_old\"],\n",
        "        advantages=rollout[\"advantages\"],\n",
        "        returns=rollout[\"returns\"],\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"step={step} rm_reward={rollout['rm_reward']:.3f} kl_mean={rollout['kl_mean']:.3f} \"\n",
        "        f\"loss={metrics['loss']:.4f} policy={metrics['policy_loss']:.4f} value={metrics['value_loss']:.4f} \"\n",
        "        f\"approx_kl={metrics['approx_kl']:.4f} clip_frac={metrics['clip_frac']:.3f}\"\n",
        "    )\n",
        "    print(\"prompt:\", rollout[\"prompt\"])\n",
        "    print(\"response:\", rollout[\"response\"])\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 简单验证\n",
        "\n",
        "训练后再看一眼输出。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: czq是谁\n",
            "A: 作为一个AI助手，我无法提供您的具体信息或身份，因此我无法回答关于“谁是cqc”的问题。请注意，任何关于个人身份的提问都是不准确和不安全的。如果您有其他问题需要帮助，请随时告诉我。\n",
            "\n",
            "Q: 请只输出数字 4，不要额外文字。2+2等于几？\n",
            "A: 2 + 2 等于 4。\n",
            "\n",
            "Q: 把“我喜欢机器学习”翻译成英文，只输出翻译。\n",
            "A: I like machine learning.\n",
            "\n",
            "Q: 请只回答：通义千问\n",
            "A: 通义千问是由阿里云研发的AI超大规模预训练模型。\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@torch.inference_mode()\n",
        "def chat(prompt: str, max_new_tokens: int = 64) -> str:\n",
        "    prompt_ids = build_prompt_input_ids(tokenizer, prompt)\n",
        "    full_ids, response_ids = sample_response(actor_critic.base, tokenizer, prompt_ids, max_new_tokens=max_new_tokens, temperature=0.7, top_p=0.9)\n",
        "    return tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "for t in train_tasks:\n",
        "    print(\"Q:\", t[\"prompt\"])\n",
        "    print(\"A:\", chat(t[\"prompt\"]))\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
