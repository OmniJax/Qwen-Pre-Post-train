{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91713121-11f0-499b-8a19-cfb66a29bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Qwen2模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1183c8-29a3-476e-84cd-51b0b1d2e93c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T07:23:21.684828Z",
     "iopub.status.busy": "2024-09-21T07:23:21.684308Z",
     "iopub.status.idle": "2024-09-21T07:23:27.803056Z",
     "shell.execute_reply": "2024-09-21T07:23:27.802461Z",
     "shell.execute_reply.started": "2024-09-21T07:23:21.684806Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 14:23:35,912 - modelscope - INFO - Creating symbolic link [D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0.5B-Instruct].\n",
      "2026-02-25 14:23:35,913 - modelscope - WARNING - Failed to create symbolic link D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0.5B-Instruct for D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0___5B-Instruct.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 14:23:39,939 - modelscope - INFO - Creating symbolic link [D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0.5B-Instruct].\n",
      "2026-02-25 14:23:39,941 - modelscope - WARNING - Failed to create symbolic link D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0.5B-Instruct for D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0___5B-Instruct.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM,AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "os.environ['MODELSCOPE_CACHE'] = r'D:\\myProject\\modelscope_hub'\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"qwen/Qwen2-0.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen2-0.5B-Instruct\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6d3612-4e69-48c3-b6a7-82f5da95ae4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T07:23:27.804133Z",
     "iopub.status.busy": "2024-09-21T07:23:27.803814Z",
     "iopub.status.idle": "2024-09-21T07:23:27.807644Z",
     "shell.execute_reply": "2024-09-21T07:23:27.807072Z",
     "shell.execute_reply.started": "2024-09-21T07:23:27.804109Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2TokenizerFast(name_or_path='D:\\myProject\\modelscope_hub\\models\\qwen\\Qwen2-0___5B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a92426f-1098-48f2-8c56-d473e5ba5d18",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T07:23:27.808639Z",
     "iopub.status.busy": "2024-09-21T07:23:27.808435Z",
     "iopub.status.idle": "2024-09-21T07:23:27.812761Z",
     "shell.execute_reply": "2024-09-21T07:23:27.812282Z",
     "shell.execute_reply.started": "2024-09-21T07:23:27.808620Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    # print(text)\n",
    "    # print('-'*30)\n",
    "    with torch.no_grad():\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "        # print(model_inputs)\n",
    "        # print('-'*30)\n",
    "\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=512\n",
    "        )\n",
    "        \n",
    "        # print('total',generated_ids)\n",
    "        # print('-'*30)\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        # print('generated',generated_ids)\n",
    "        # print('-'*30)\n",
    "\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        torch.cuda.empty_cache()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45aafdf-6aed-47f3-84a5-c06ce45aaab4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T07:23:27.813642Z",
     "iopub.status.busy": "2024-09-21T07:23:27.813395Z",
     "iopub.status.idle": "2024-09-21T07:23:29.135149Z",
     "shell.execute_reply": "2024-09-21T07:23:29.134650Z",
     "shell.execute_reply.started": "2024-09-21T07:23:27.813622Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是来自阿里云的大规模语言模型，我叫通义千问。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"你是谁？\"\n",
    "chat(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c520020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 等于 4。\n",
      "22 + 22 equals 44。\n",
      "3 + 3 等于 6。\n"
     ]
    }
   ],
   "source": [
    "print(chat('2+2等于几'))\n",
    "print(chat('22+22等于几'))\n",
    "print(chat('3+3等于几'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_chat(prompts):\n",
    "    \"\"\"\n",
    "    批量推理实现，不用for循环。\n",
    "    输入: prompts: List[str]\n",
    "    输出: responses: List[str]\n",
    "    \"\"\"\n",
    "    batch_messages = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": p},\n",
    "        ]\n",
    "        for p in prompts\n",
    "    ]\n",
    "    texts = tokenizer.apply_chat_template(\n",
    "        batch_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # texts: List[str]，适合直接 batch\n",
    "    with torch.no_grad():\n",
    "        model_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=512\n",
    "        )\n",
    "        # 对于批量，截取每个结果的 generated 部分\n",
    "        input_lens = [len(ids) for ids in model_inputs.input_ids]\n",
    "        gen_ids = []\n",
    "        for idx, output_ids in enumerate(generated_ids):\n",
    "            gen_ids.append(output_ids[input_lens[idx]:])\n",
    "        responses = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        return responses\n",
    "\n",
    "# 示例:\n",
    "batch_prompts = [\"2+2等于几\", \"22+22等于几\", \"3+3等于几\"]\n",
    "results = batch_chat(batch_prompts)\n",
    "for p, r in zip(batch_prompts, results):\n",
    "    print(f\"Q: {p}\\nA: {r}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aaa570e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><|endoftext|><|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n2+2等于几<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n22+22等于几<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " '<|endoftext|><|endoftext|><|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n3+3等于几<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n你是什么模型<|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_prompts = [\"2+2等于几\", \"22+22等于几\", \"3+3等于几\",'你是什么模型']\n",
    "batch_messages = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": p},\n",
    "    ]\n",
    "    for p in batch_prompts\n",
    "]\n",
    "texts = tokenizer.apply_chat_template(\n",
    "    batch_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer(texts, return_tensors=\"pt\",padding=True,padding_side='left').to(device)\n",
    "(tokenizer.batch_decode(model_inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "460d98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "570a62ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system\\nYou are a helpful assistant.\\nuser\\n2+2等于几\\nassistant\\n',\n",
       " 'system\\nYou are a helpful assistant.\\nuser\\n22+22等于几\\nassistant\\n33333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333',\n",
       " 'system\\nYou are a helpful assistant.\\nuser\\n3+3等于几\\nassistant\\n3+3等于3333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333',\n",
       " 'system\\nYou are a helpful assistant.\\nuser\\n你是什么模型\\nassistant\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_ids,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8724da40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '33333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333',\n",
       " '3+3等于3333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333',\n",
       " '']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lens = [len(ids) for ids in model_inputs.input_ids]\n",
    "gen_ids = []\n",
    "for idx, output_ids in enumerate(generated_ids):\n",
    "    gen_ids.append(output_ids[input_lens[idx]:])\n",
    "responses = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ddde94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ccd478e-5354-43a9-8d4a-2e4e1a0b46e5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T07:23:29.136185Z",
     "iopub.status.busy": "2024-09-21T07:23:29.135930Z",
     "iopub.status.idle": "2024-09-21T07:23:29.142901Z",
     "shell.execute_reply": "2024-09-21T07:23:29.142420Z",
     "shell.execute_reply.started": "2024-09-21T07:23:29.136164Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练数据预处理方法\n",
    "def preprocess(tokenizer,batch_messages):\n",
    "    input_list=[]\n",
    "    target_list=[]\n",
    "    \n",
    "    im_start=tokenizer('<|im_start|>').input_ids\n",
    "    im_end=tokenizer('<|im_end|>').input_ids\n",
    "    newline=tokenizer('\\n').input_ids\n",
    "    pad=tokenizer('<|endoftext|>').input_ids\n",
    "    ignore=[-100]\n",
    "    \n",
    "    for group in batch_messages:\n",
    "        input_ids=[]\n",
    "        target_ids=[]\n",
    "        for msg in group:\n",
    "            role=tokenizer(msg['role']).input_ids\n",
    "            content=tokenizer(msg['content']).input_ids\n",
    "            if msg['role'] in ['system','user']:\n",
    "                ignore_parts=role+newline+content\n",
    "                input_ids+=im_start+ignore_parts+im_end+newline\n",
    "                target_ids+=im_start+ignore*len(ignore_parts)+im_end+newline\n",
    "            else:\n",
    "                ignore_parts=role+newline\n",
    "                input_ids+=im_start+ignore_parts+content+im_end+newline\n",
    "                target_ids+=im_start+ignore*len(ignore_parts)+content+im_end+newline\n",
    "        input_list.append(input_ids)\n",
    "        target_list.append(target_ids)\n",
    "    \n",
    "    # padding\n",
    "    max_len=max([len(ids) for ids in input_list])\n",
    "    for input_ids,target_ids in zip(input_list,target_list):\n",
    "        input_ids+=pad*(max_len-len(input_ids))\n",
    "        target_ids+=ignore*(max_len-len(target_ids))\n",
    "    batch_input_ids=torch.tensor(input_list,dtype=torch.long)\n",
    "    batch_target_ids=torch.tensor(target_list,dtype=torch.long)\n",
    "    batch_mask=batch_input_ids.ne(pad[0]).type(torch.long)\n",
    "    return batch_input_ids,batch_target_ids,batch_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4508416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"2+2等于几\"\n",
    "messages = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": '2+2等于5。'},\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": '3+3等于7。'},\n",
    "    ]\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "batch_input_ids,batch_target_ids,batch_mask=preprocess(tokenizer,messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b166dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "221b021f-eccd-40d9-bf4e-d3b39b5bfd1d",
   "metadata": {},
   "source": [
    "# 训练数据测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5433da08-9def-4c3b-982f-d49e22161e96",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T07:23:39.399356Z",
     "iopub.status.busy": "2024-09-21T07:23:39.399015Z",
     "iopub.status.idle": "2024-09-21T07:23:39.470823Z",
     "shell.execute_reply": "2024-09-21T07:23:39.470296Z",
     "shell.execute_reply.started": "2024-09-21T07:23:39.399333Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([2, 31, 151936])\n",
      "targets: torch.Size([2, 31])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_input_ids,batch_target_ids,batch_mask=preprocess(tokenizer,messages)\n",
    "model_outputs=model(batch_input_ids.to(device))\n",
    "\n",
    "output_tokens=model_outputs.logits.argmax(dim=-1)\n",
    "\n",
    "logits=model_outputs.logits[:,:-1,:]\n",
    "targets=batch_target_ids[:,1:].to(device)\n",
    "print('logits:',logits.shape) # 模型输出\n",
    "print('targets:',targets.shape) # 拟合目标\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a08338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(2.2188, device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 损失\n",
    "loss_fn=CrossEntropyLoss()\n",
    "loss=loss_fn(logits.reshape(-1,logits.size(2)),targets.reshape(-1))\n",
    "print('loss:',loss)\n",
    "\n",
    "# 优化器\n",
    "optimizer=torch.optim.SGD(model.parameters())\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 求梯度\n",
    "loss.backward()\n",
    "\n",
    "# 梯度下降\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f72c302-f842-48f5-95c1-89d1101a2737",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-09-21T07:23:45.741993Z",
     "iopub.status.busy": "2024-09-21T07:23:45.741624Z",
     "iopub.status.idle": "2024-09-21T07:23:45.885587Z",
     "shell.execute_reply": "2024-09-21T07:23:45.885085Z",
     "shell.execute_reply.started": "2024-09-21T07:23:45.741969Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'33333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat('2+2等于')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab10dc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3+3等于3333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat('2+3等于')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970e72f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
